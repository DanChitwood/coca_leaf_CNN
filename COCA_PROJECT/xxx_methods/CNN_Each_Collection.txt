### Methods: Morphometric Feature Extraction, Data Augmentation, and Convolutional Neural Network Classification

This study employed a comprehensive computational pipeline for the morphometric analysis and classification of leaf shapes from two distinct collections, "Cultivated 1st" and "Cultivated 2nd." The pipeline encompassed initial shape representation, synthetic data augmentation, Euler Characteristic Transform (ECT) feature extraction, and convolutional neural network (CNN) model training and evaluation. All computational steps were performed using Python (version X.X) with key libraries including NumPy (version X.X), Pandas (version X.X), Scikit-learn (version X.X), Matplotlib (version X.X), Seaborn (version X.X), OpenCV (version X.X), Pillow (version X.X), and PyTorch (version X.X). For reproducibility, a global random seed (42) was consistently applied across all random operations within NumPy, Python's `random` module, and PyTorch.

#### A. Initial Shape Representation and Principal Component Analysis (PCA)

Leaf contours were initially represented by 99 pseudo-landmarks, derived from a prior landmarking procedure. These 2D coordinate pairs were flattened into 198-dimensional vectors ($99 \times 2$ coordinates) to serve as the foundational shape data. Principal Component Analysis (PCA) was then applied to this high-dimensional coordinate space to reduce dimensionality while capturing the primary modes of morphological variation. Separate PCA models (including components, mean, and explained variance) were trained and saved for each collection ("Cultivated 1st" and "Cultivated 2nd"). These pre-trained PCA models were subsequently loaded for inverse transformations, enabling the reconstruction of 2D leaf contours from their PCA scores.

#### B. Synthetic Data Generation via SMOTE-like Augmentation

To address potential class imbalances and augment the datasets, a Synthetic Minority Over-sampling Technique (SMOTE)-like approach was implemented directly within the respective PCA feature spaces for each collection. For each class, synthetic samples were generated by randomly selecting an existing real sample and one of its $k=5$ nearest neighbors within the same class in the PCA space. A new synthetic sample's PCA scores were then created by interpolating between these two real samples along the line segment connecting them, using a random interpolation factor ($\alpha$) uniformly sampled between 0 and 1. This process was iteratively repeated until each class contained a target of 400 synthetic samples. The PCA scores of these synthetic samples were then inverse-transformed using the corresponding collection's PCA model to reconstruct their 2D contour points in the original coordinate space. This augmentation strategy was applied independently to both the "Cultivated 1st" and "Cultivated 2nd" datasets, significantly expanding the training data for robust model development.

#### C. Euler Characteristic Transform (ECT) Calculation and Image Generation

The Euler Characteristic Transform (ECT) was employed to derive a rotation- and scale-invariant morphometric feature for CNN input. For every real and synthetic leaf shape, the following steps were performed:

1.  **Contour Reconstruction:** The flattened 198-dimensional coordinate vectors (either original real data or inverse-transformed synthetic data) were reshaped back into 99 (x, y) contour points.
2.  **ECT Internal Normalization:** Each 2D contour was then processed by the ECT library's internal normalization procedures. This involved centering the coordinates around the origin, performing a Procrustes-like alignment to achieve a standardized orientation and size, and scaling the contour such that it was contained within a bounding circle of radius 1. It is important to note that no additional random rotations were applied during this step, ensuring that the intrinsic shape alignment for ECT remained consistent across all samples.
3.  **ECT Calculation:** The ECT was computed for the normalized contour using **180 radial directions** and **180 distance thresholds**. The distance thresholds were linearly spaced from 0 to the bounding radius (1). This computation yielded a **180x180 matrix of ECT values** for each leaf shape.

To prepare these ECT features for CNN input, the 180x180 ECT matrices were rendered as 256x256 single-channel grayscale images. To ensure consistent pixel intensity scaling, a global minimum and maximum ECT value was determined by analyzing all ECT results (from both real and synthetic samples) within each specific dataset ("Cultivated 1st" or "Cultivated 2nd"). This dataset-specific global range was then used to normalize the pixel intensities of all ECT images within that dataset to a consistent grayscale range. The final CNN input for each leaf consisted of a **256x256 two-channel image**: one channel containing the grayscale ECT image and the second channel containing a binary shape mask (also 256x256), representing the presence or absence of leaf material. For visualization and quality control purposes, combined images were also generated, overlaying a black leaf outline on a reverse grayscale ECT background with a white facecolor.

#### D. Convolutional Neural Network (CNN) Architecture and Training

Separate CNN models were trained and evaluated for each collection ("Cultivated 1st" and "Cultivated 2nd") to predict the cultigen class label. The CNN architecture was implemented using PyTorch and designed as a sequential model:

* **Input Layer:** Accepts 256x256 images with 2 input channels (ECT and shape mask).
* **Feature Extraction Layers:** Consists of three sequential blocks, each comprising:
    * `Conv2d` layer with a 3x3 kernel and 1-pixel padding, increasing filter counts (32, 64, 128).
    * `BatchNorm2d` layer for feature normalization.
    * `ReLU` activation function for non-linearity.
    * `MaxPool2d` layer with a 2x2 kernel and stride 2 for down-sampling.
* **Classifier Head:**
    * `Flatten` layer to convert 2D feature maps into a 1D vector.
    * `Linear` (dense) layer with 512 units, followed by `ReLU` activation.
    * `Dropout` layer with a rate of 0.5 to prevent overfitting.
    * Final `Linear` layer with the number of units equal to the number of unique classes in the respective dataset, using a `softmax` activation (implicitly applied by `CrossEntropyLoss`).

Model training employed a 5-fold stratified cross-validation strategy. For each fold, the validation set consisted *exclusively* of real leaf samples from that fold's split. The training set for each fold comprised *all synthetic samples* combined with the real leaf samples from the training split of that fold. This approach leveraged the augmented data while ensuring that model performance was rigorously evaluated on unseen real data. Class weights, computed using `sklearn.utils.class_weight.compute_class_weight('balanced')` based on the distribution of all training labels (real + synthetic), were applied to the `CrossEntropyLoss` criterion to mitigate the impact of class imbalance. The Adam optimizer was used with a learning rate of 0.001. A `ReduceLROnPlateau` scheduler monitored validation loss, reducing the learning rate by a factor of 0.1 if validation loss did not improve for 5 epochs. Early stopping was implemented with a patience of 10 epochs; training for a fold would cease if validation loss did not improve for this duration. For each fold, the model weights achieving the best validation loss were saved. Final predictions for the ensemble were derived by averaging the raw logits from the best models of all 5 folds, then applying an argmax to determine the final class.

#### E. Model Evaluation and Interpretability

Model performance was evaluated on the real samples using standard classification metrics: accuracy, precision, recall, and F1-score. Classification reports and confusion matrices (both raw and normalized) were generated and saved for each trained model.

For model interpretability, Grad-CAM (Gradient-weighted Class Activation Mapping) visualizations were generated. Grad-CAM heatmaps were computed by targeting the output of the final convolutional layer (`cam_model.features[8]`) of the trained CNN model (specifically, the model from Fold 0 was used for this visualization purpose). For each class, an average Grad-CAM heatmap was calculated by summing the heatmaps from 5 randomly selected real samples belonging to that class and then normalizing the sum. These average heatmaps were then overlaid onto the corresponding ECT channel of an example image (displayed in reverse grayscale with a white background and black contour outline) to visually highlight the regions of the ECT feature space that were most influential in the model's classification decision for each class.