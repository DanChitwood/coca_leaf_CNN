### Methods: Morphometric Feature Extraction, Data Augmentation, and CNN Architecture

To analyze and classify leaf morphology, a comprehensive pipeline was developed, encompassing morphometric feature extraction using the Euler Characteristic Transform (ECT), data augmentation techniques, and a custom Convolutional Neural Network (CNN) architecture designed to process these specialized features.

#### A. Initial Shape Representation and PCA

Leaf contours, previously acquired and pre-processed through a landmarking procedure, consist of 99 pseudo-landmarks (2D points). These 2D points were flattened into 198-dimensional vectors ($99 \times 2$ coordinates) to form the basis of the shape dataset. Principal Component Analysis (PCA) was then applied to this high-dimensional coordinate space. PCA serves to reduce dimensionality while capturing the principal modes of morphological variation within the leaf shapes, effectively identifying the most significant features that describe shape differences. The trained PCA model, including its components, mean, and explained variance, was saved to enable inverse transformations for reconstructing shapes from PCA scores.

#### B. Synthetic Data Generation via SMOTE-like Oversampling

To mitigate potential class imbalances within the dataset and to augment the training data, a Synthetic Minority Over-sampling Technique (SMOTE)-like approach was implemented directly in the PCA feature space. For each distinct class of leaf shapes, the method proceeded as follows:
1.  **Neighbor Identification:** For each sample in a given class, its $k=5$ nearest neighbors within the same class were identified in the PCA feature space.
2.  **Synthetic Sample Creation:** A synthetic sample was generated by randomly selecting an existing sample from the class and then randomly selecting one of its identified nearest neighbors. A new synthetic data point was then created by interpolating between these two real samples along the line segment connecting them. The interpolation factor ($\alpha$) was a random value between 0 and 1.
This process was iteratively repeated until each class contained a target of 400 synthetic samples, significantly expanding the dataset's diversity and size for robust model training.

#### C. Euler Characteristic Transform (ECT) Calculation and Normalization

The primary morphometric feature used as input for the CNN was derived using the Euler Characteristic Transform (ECT). The ECT provides a powerful, rotation- and scale-invariant representation of closed contours and shapes, making it ideal for shape analysis where absolute position and size are not relevant to classification. For each original (real) and newly generated (synthetic) leaf shape, the ECT calculation involved the following sequential steps:

1.  **Inverse PCA Transformation:** For synthetic samples, the generated PCA scores were inverse-transformed back into the original 198-dimensional flattened coordinate space, reconstructing the 99 (x, y) contour points that define the leaf shape. Original real samples already possessed these 99 (x, y) contour points.
2.  **Embedded Graph Construction:** Each set of 99 (x, y) contour points was used to construct an `EmbeddedGraph` object, which is the required input format for the `ect` library.
3.  **ECT Internal Normalization:** The `EmbeddedGraph` underwent a series of intrinsic normalizations performed by the ECT library:
    * **Centering:** The coordinates of the contour were shifted such that their centroid coincided with the origin (0,0).
    * **Transformation (Procrustes-like Alignment):** The shape was rotated and scaled to achieve a standardized orientation and size. This step effectively aligns shapes to a common reference while preserving their intrinsic morphological characteristics, making them comparable regardless of their original acquisition orientation or scale.
    * **Scaling:** The normalized contour was then uniformly scaled such that it was completely contained within a bounding circle of `BOUND_RADIUS = 1`.
    It is critical to note that for all data (both real and synthetic), random rotations were explicitly *not* applied during this processing step, ensuring that the Procrustes-like alignment and subsequent ECT values remained consistent and comparable across all samples.
4.  **ECT Calculation:** Finally, the ECT was calculated for the normalized contour. This calculation was performed with **180 radial directions** and **180 distance thresholds**. The distance thresholds were linearly spaced from 0 to the `BOUND_RADIUS`. This process yielded a **180x180 matrix of ECT values** for each leaf shape, where each entry corresponds to the ECT value at a specific radial direction and distance threshold.

#### D. Image Generation for CNN Input

To make the ECT features compatible with a CNN, the 180x180 ECT matrices were rendered as single-channel grayscale images. To ensure consistent intensity scaling across all generated images (for both real and synthetic samples), a global minimum (`GLOBAL_ECT_MIN`) and maximum (`GLOBAL_ECT_MAX`) ECT value was determined by processing all ECT results from the combined real and synthetic datasets. This global range was then used to normalize the pixel intensities of all ECT images, mapping the ECT values to a consistent grayscale range suitable for display and CNN input.

The resulting **radial ECT images were saved at a fixed resolution of 256x256 pixels**. These 256x256 single-channel images directly serve as one of the inputs to the CNN model. In addition to ECT images, binary shape masks (also 256x256 pixels, indicating the presence or absence of leaf material) and combined visualizations (ECT image with an overlaid outline of the corresponding leaf contour) were generated. The latter were primarily used for quality control and visual verification of the processed data and transformations.

#### E. Convolutional Neural Network (CNN) Architecture

The CNN architecture designed for this task processes the generated 256x256 single-channel radial ECT images. The architecture is based on a sequential model, suitable for image classification, and is typically constructed as follows:

* **Input Layer:** The network accepts input images of shape (256, 256, 1), corresponding to the height, width, and single channel (grayscale) of the ECT images.
* **Convolutional Blocks:** The architecture utilizes a series of convolutional blocks, each typically consisting of:
    * **Convolutional Layer (`Conv2D`):** Applies filters to the input to learn spatial features.
    * **Activation Function (`ReLU`):** Introduces non-linearity to the model, allowing it to learn more complex patterns.
    * **Batch Normalization (`BatchNormalization`):** Normalizes the activations of the previous layer, reducing internal covariate shift and improving training stability.
    * **Max Pooling Layer (`MaxPooling2D`):** Down-samples the feature maps, reducing spatial dimensions and computational complexity while retaining important features.
* **Dropout Layers:** Randomly sets a fraction of input units to zero at each update during training. This technique helps prevent overfitting by reducing complex co-adaptations of neurons, making the model more robust.
* **Flatten Layer:** Converts the 2D feature maps from the final convolutional block into a 1D vector, preparing the data for the fully connected layers.
* **Dense (Fully Connected) Layers:** These layers perform classification based on the high-level features extracted by the convolutional layers.
    * One or more hidden dense layers with `ReLU` activation are typically used to learn complex relationships from the flattened features.
    * The final dense layer's number of units corresponds to the total number of distinct output classes. It employs a `softmax` activation function to output a probability distribution over the classes, indicating the model's confidence for each class.

The specific configuration of filters, kernel sizes, strides, and dropout rates within each layer are hyperparameters that are tuned during the model development and training phases to optimize the model's performance on the given morphometric data. The model is compiled with an appropriate optimizer (e.g., Adam) and a loss function suitable for multi-class classification (e.g., categorical cross-entropy).