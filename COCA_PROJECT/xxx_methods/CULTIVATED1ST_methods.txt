
## Methods: CULTIVATED1ST Experiment Analysis

---

This study employed a deep learning approach to classify plant leaf images from the CULTIVATED1ST experiment, utilizing a custom Convolutional Neural Network (CNN) architecture trained with both real and synthetically generated data. The analysis pipeline encompassed data preparation, model training via stratified K-fold cross-validation, and performance evaluation, with a focus on reproducibility and model interpretability through Grad-CAM visualizations.

### Dataset Preparation

The CULTIVATED1ST experiment images were initially processed to generate a two-channel input format optimized for CNN analysis. This involved extracting two distinct channels for each image: a **binary mask** representing the leaf's silhouette (Channel 0) and an **Euler Characteristic Transform (ECT) image** (Channel 1). The ECT image was derived from the raw RGB input using techniques from computational topology, capturing specific geometric and topological features of the leaf structure. All images were then resized to a uniform dimension of 256x256 pixels.

To augment the limited real image data and improve model generalization, **synthetic leaf images** were generated primarily through advanced data augmentation techniques applied to existing real leaf data. This process involved applying various transformations (e.g., rotations, flips, scaling, translations) to the paired ECT and mask images. The final dataset comprised both real and synthetic images, each with two channels (mask and ECT) and corresponding class labels. Data was stored in a pickled format (`.pkl`) at `/Users/chitwoo9/Desktop/COCA_PROJECT/analysis/CULTIVATED1ST/02_synthetic_leaf_data/final_cnn_dataset.pkl`, containing the image arrays, encoded labels, a flag indicating real or synthetic origin, class names, image size, and number of channels.

Class labels were **integer encoded** for compatibility with the neural network's `CrossEntropyLoss` function. To address potential class imbalance, **class weights** were computed based on the 'balanced' distribution of labels across the combined real and synthetic training set. These weights were subsequently applied to the loss function during model training.

### Convolutional Neural Network Architecture

A custom CNN, named `LeafCNN`, was designed for image classification. The architecture consisted of two main components:

1.  **Feature Extractor**: This sequential block comprised three convolutional layers, each followed by a **Batch Normalization** layer, a ReLU activation function, and a 2x2 Max Pooling layer. The use of Batch Normalization was intended to stabilize training and accelerate convergence by normalizing activations. The convolutional layers progressively extract hierarchical features, with filter counts increasing from 32 to 64 to 128.
2.  **Classifier**: The features extracted by the convolutional layers were flattened and passed through a fully connected network. This included a linear layer mapping to 512 hidden units, a ReLU activation, a 50% **Dropout** layer for regularization to prevent overfitting, and a final linear output layer with `num_classes` units, corresponding to the total number of unique plant leaf categories. The `flattened_size` of the features was dynamically calculated during model initialization to ensure compatibility with the first linear layer, accommodating varying input image dimensions.

The model was instantiated with `num_input_channels=2`, reflecting the mask and ECT channels.

### Model Training and Evaluation

Training was conducted using a **5-fold stratified cross-validation** strategy. Critically, the stratification was performed exclusively on the **real samples** of the CULTIVATED1ST dataset. For each fold, the training set consisted of all synthetic data combined with the real data from the current fold's training split. The validation set, however, was composed **solely of real data** from that fold's validation split. This approach ensured that model performance evaluation directly reflected its capability on authentic, unseen plant images, while leveraging synthetic data for robust training.

The model for each fold was optimized using the **Adam optimizer** with a learning rate of 0.001. The **CrossEntropyLoss** criterion was employed, incorporating the pre-calculated **class weights** to mitigate the impact of imbalanced class distributions. A **ReduceLROnPlateau scheduler** was implemented to dynamically adjust the learning rate, reducing it by a factor of 0.1 if the validation loss did not improve for 5 consecutive epochs, thereby aiding convergence and preventing overshooting optimal weights. **Early stopping** was also integrated, terminating training for a fold if the validation loss failed to improve for 10 consecutive epochs, and restoring the model weights that yielded the best validation loss.

Upon completion of training for all 5 folds, an **ensemble prediction** was performed. The logits (raw outputs) from each fold's best-performing model were collected for all real samples. These logits were then averaged across all folds, and the final class predictions were determined by taking the `argmax` of these averaged logits. This ensemble approach aimed to reduce variance and improve overall generalization.

Model performance was rigorously evaluated on the **entire set of real samples** using standard classification metrics, including overall **accuracy**, and per-class **precision, recall, and F1-score**. A detailed **classification report** and both raw and **normalized confusion matrices** were generated to provide comprehensive insights into the model's performance and identify misclassification patterns. These metrics were saved in JSON and image formats (PNG) within the `trained_models/metrics_output/` directory, for record-keeping. Best model checkpoints for each fold were saved as `.pth` files in `trained_models/`.

### Reproducibility and Interpretability

To ensure the reproducibility of results, a **fixed random seed (42)** was set at the outset of the script, influencing NumPy, and PyTorch operations (including CPU and MPS-related determinism settings).

Model interpretability was explored using **Grad-CAM (Gradient-weighted Class Activation Mapping)**. Grad-CAM heatmaps were generated for each class by averaging the activation maps of 5 randomly selected real samples per class from the training fold 0's best model. The last convolutional layer (`cam_model.features[-3]`) was chosen as the target layer for visualization. The generated heatmaps were then overlaid onto the ECT channel of the original images, presented against a black background, to visually highlight regions of input data that were most influential in the model's predictions for each class. These visualizations provide insight into the features the CNN learns to associate with specific plant leaf categories and were saved as PNG images within the `trained_models/grad_cam_images/` directory.
