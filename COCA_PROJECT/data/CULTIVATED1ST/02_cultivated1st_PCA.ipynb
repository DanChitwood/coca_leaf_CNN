{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1709dd-aa7f-48a8-bb83-b7b757f43a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving outputs to directory: ./03_morphometrics_output_cultivated1st/\n",
      "Metadata loaded from: ./01_cultivated1st_landmarks.csv\n",
      "First 5 rows of loaded metadata:\n",
      "         file variety       full_name  type  plant leaf  px_cm  base_x  \\\n",
      "0  BRO4_e.tif     BRO  boliviana roja  coca      4    e  28.42    95.5   \n",
      "1  BRO4_d.tif     BRO  boliviana roja  coca      4    d  28.42    85.0   \n",
      "2  BRO4_c.tif     BRO  boliviana roja  coca      4    c  28.42    78.0   \n",
      "3  BRO4_b.tif     BRO  boliviana roja  coca      4    b  28.42   104.0   \n",
      "4  BRO4_a.tif     BRO  boliviana roja  coca      4    a  28.42    75.0   \n",
      "\n",
      "   base_y  tip_x  tip_y  \n",
      "0   194.5  302.5  197.5  \n",
      "1   199.5  315.0  201.5  \n",
      "2   202.5  321.0  204.0  \n",
      "3   192.0  296.5  201.0  \n",
      "4   187.5  325.5  202.5  \n",
      "Found 319 image files to process from metadata.\n",
      "\n",
      "--- Preprocessing Images and Interpolating Pseudo-Landmarks ---\n",
      "\n",
      "--- Calculating GPA Mean ---\n",
      "--- Aligning Leaves to GPA Mean ---\n",
      "--- Visualizing GPA Aligned Shapes ---\n",
      "GPA mean shape plot saved to ./03_morphometrics_output_cultivated1st/gpa_mean_shape.png\n",
      "\n",
      "--- Performing Full PCA and Generating Explained Variance Report ---\n",
      "PC1: 65.16%, 65.16%\n",
      "PC2: 16.16%, 81.32%\n",
      "PC3: 7.69%, 89.01%\n",
      "PC4: 3.37%, 92.38%\n",
      "PC5: 1.48%, 93.86%\n",
      "PC6: 1.33%, 95.19%\n",
      "PC7: 0.93%, 96.12%\n",
      "PC8: 0.62%, 96.74%\n",
      "PC9: 0.57%, 97.31%\n",
      "PC10: 0.39%, 97.7%\n",
      "PC11: 0.29%, 97.99%\n",
      "PC12: 0.26%, 98.25%\n",
      "PC13: 0.24%, 98.49%\n",
      "PC14: 0.17%, 98.67%\n",
      "PC15: 0.13%, 98.79%\n",
      "PC16: 0.11%, 98.9%\n",
      "PC17: 0.09%, 98.99%\n",
      "PC18: 0.07%, 99.07%\n",
      "PC19: 0.06%, 99.12%\n",
      "PC20: 0.05%, 99.17%\n",
      "PC21: 0.05%, 99.22%\n",
      "PC22: 0.04%, 99.26%\n",
      "PC23: 0.04%, 99.3%\n",
      "PC24: 0.03%, 99.33%\n",
      "PC25: 0.03%, 99.37%\n",
      "PC26: 0.03%, 99.4%\n",
      "PC27: 0.03%, 99.43%\n",
      "PC28: 0.03%, 99.45%\n",
      "PC29: 0.02%, 99.48%\n",
      "PC30: 0.02%, 99.5%\n",
      "PC31: 0.02%, 99.52%\n",
      "PC32: 0.02%, 99.54%\n",
      "PC33: 0.02%, 99.56%\n",
      "PC34: 0.02%, 99.57%\n",
      "PC35: 0.02%, 99.59%\n",
      "PC36: 0.01%, 99.6%\n",
      "PC37: 0.01%, 99.62%\n",
      "PC38: 0.01%, 99.63%\n",
      "PC39: 0.01%, 99.64%\n",
      "PC40: 0.01%, 99.65%\n",
      "PC41: 0.01%, 99.67%\n",
      "PC42: 0.01%, 99.68%\n",
      "PC43: 0.01%, 99.69%\n",
      "PC44: 0.01%, 99.7%\n",
      "PC45: 0.01%, 99.71%\n",
      "PC46: 0.01%, 99.72%\n",
      "PC47: 0.01%, 99.73%\n",
      "PC48: 0.01%, 99.74%\n",
      "PC49: 0.01%, 99.74%\n",
      "PC50: 0.01%, 99.75%\n",
      "PC51: 0.01%, 99.76%\n",
      "PC52: 0.01%, 99.77%\n",
      "PC53: 0.01%, 99.78%\n",
      "PC54: 0.01%, 99.78%\n",
      "PC55: 0.01%, 99.79%\n",
      "PC56: 0.01%, 99.8%\n",
      "PC57: 0.01%, 99.8%\n",
      "PC58: 0.01%, 99.81%\n",
      "PC59: 0.01%, 99.82%\n",
      "PC60: 0.01%, 99.82%\n",
      "PC61: 0.01%, 99.83%\n",
      "PC62: 0.01%, 99.83%\n",
      "PC63: 0.01%, 99.84%\n",
      "PC64: 0.01%, 99.85%\n",
      "PC65: 0.01%, 99.85%\n",
      "PC66: 0.0%, 99.86%\n",
      "PC67: 0.0%, 99.86%\n",
      "PC68: 0.0%, 99.86%\n",
      "PC69: 0.0%, 99.87%\n",
      "PC70: 0.0%, 99.87%\n",
      "PC71: 0.0%, 99.88%\n",
      "PC72: 0.0%, 99.88%\n",
      "PC73: 0.0%, 99.89%\n",
      "PC74: 0.0%, 99.89%\n",
      "PC75: 0.0%, 99.89%\n",
      "PC76: 0.0%, 99.9%\n",
      "PC77: 0.0%, 99.9%\n",
      "PC78: 0.0%, 99.91%\n",
      "PC79: 0.0%, 99.91%\n",
      "PC80: 0.0%, 99.91%\n",
      "PC81: 0.0%, 99.92%\n",
      "PC82: 0.0%, 99.92%\n",
      "PC83: 0.0%, 99.92%\n",
      "PC84: 0.0%, 99.92%\n",
      "PC85: 0.0%, 99.93%\n",
      "PC86: 0.0%, 99.93%\n",
      "PC87: 0.0%, 99.93%\n",
      "PC88: 0.0%, 99.94%\n",
      "PC89: 0.0%, 99.94%\n",
      "PC90: 0.0%, 99.94%\n",
      "PC91: 0.0%, 99.94%\n",
      "PC92: 0.0%, 99.95%\n",
      "PC93: 0.0%, 99.95%\n",
      "PC94: 0.0%, 99.95%\n",
      "PC95: 0.0%, 99.95%\n",
      "PC96: 0.0%, 99.95%\n",
      "PC97: 0.0%, 99.96%\n",
      "PC98: 0.0%, 99.96%\n",
      "PC99: 0.0%, 99.96%\n",
      "PC100: 0.0%, 99.96%\n",
      "PC101: 0.0%, 99.96%\n",
      "PC102: 0.0%, 99.97%\n",
      "PC103: 0.0%, 99.97%\n",
      "PC104: 0.0%, 99.97%\n",
      "PC105: 0.0%, 99.97%\n",
      "PC106: 0.0%, 99.97%\n",
      "PC107: 0.0%, 99.97%\n",
      "PC108: 0.0%, 99.98%\n",
      "PC109: 0.0%, 99.98%\n",
      "PC110: 0.0%, 99.98%\n",
      "PC111: 0.0%, 99.98%\n",
      "PC112: 0.0%, 99.98%\n",
      "PC113: 0.0%, 99.98%\n",
      "PC114: 0.0%, 99.98%\n",
      "PC115: 0.0%, 99.98%\n",
      "PC116: 0.0%, 99.99%\n",
      "PC117: 0.0%, 99.99%\n",
      "PC118: 0.0%, 99.99%\n",
      "PC119: 0.0%, 99.99%\n",
      "PC120: 0.0%, 99.99%\n",
      "PC121: 0.0%, 99.99%\n",
      "PC122: 0.0%, 99.99%\n",
      "PC123: 0.0%, 99.99%\n",
      "PC124: 0.0%, 99.99%\n",
      "PC125: 0.0%, 99.99%\n",
      "PC126: 0.0%, 99.99%\n",
      "PC127: 0.0%, 99.99%\n",
      "PC128: 0.0%, 99.99%\n",
      "PC129: 0.0%, 99.99%\n",
      "PC130: 0.0%, 99.99%\n",
      "PC131: 0.0%, 100.0%\n",
      "PC132: 0.0%, 100.0%\n",
      "PC133: 0.0%, 100.0%\n",
      "PC134: 0.0%, 100.0%\n",
      "PC135: 0.0%, 100.0%\n",
      "PC136: 0.0%, 100.0%\n",
      "PC137: 0.0%, 100.0%\n",
      "PC138: 0.0%, 100.0%\n",
      "PC139: 0.0%, 100.0%\n",
      "PC140: 0.0%, 100.0%\n",
      "PC141: 0.0%, 100.0%\n",
      "PC142: 0.0%, 100.0%\n",
      "PC143: 0.0%, 100.0%\n",
      "PC144: 0.0%, 100.0%\n",
      "PC145: 0.0%, 100.0%\n",
      "PC146: 0.0%, 100.0%\n",
      "PC147: 0.0%, 100.0%\n",
      "PC148: 0.0%, 100.0%\n",
      "PC149: 0.0%, 100.0%\n",
      "PC150: 0.0%, 100.0%\n",
      "PC151: 0.0%, 100.0%\n",
      "PC152: 0.0%, 100.0%\n",
      "PC153: 0.0%, 100.0%\n",
      "PC154: 0.0%, 100.0%\n",
      "PC155: 0.0%, 100.0%\n",
      "PC156: 0.0%, 100.0%\n",
      "PC157: 0.0%, 100.0%\n",
      "PC158: 0.0%, 100.0%\n",
      "PC159: 0.0%, 100.0%\n",
      "PC160: 0.0%, 100.0%\n",
      "PC161: 0.0%, 100.0%\n",
      "PC162: 0.0%, 100.0%\n",
      "PC163: 0.0%, 100.0%\n",
      "PC164: 0.0%, 100.0%\n",
      "PC165: 0.0%, 100.0%\n",
      "PC166: 0.0%, 100.0%\n",
      "PC167: 0.0%, 100.0%\n",
      "PC168: 0.0%, 100.0%\n",
      "PC169: 0.0%, 100.0%\n",
      "PC170: 0.0%, 100.0%\n",
      "PC171: 0.0%, 100.0%\n",
      "PC172: 0.0%, 100.0%\n",
      "PC173: 0.0%, 100.0%\n",
      "PC174: 0.0%, 100.0%\n",
      "PC175: 0.0%, 100.0%\n",
      "PC176: 0.0%, 100.0%\n",
      "PC177: 0.0%, 100.0%\n",
      "PC178: 0.0%, 100.0%\n",
      "PC179: 0.0%, 100.0%\n",
      "PC180: 0.0%, 100.0%\n",
      "PC181: 0.0%, 100.0%\n",
      "PC182: 0.0%, 100.0%\n",
      "PC183: 0.0%, 100.0%\n",
      "PC184: 0.0%, 100.0%\n",
      "PC185: 0.0%, 100.0%\n",
      "PC186: 0.0%, 100.0%\n",
      "PC187: 0.0%, 100.0%\n",
      "PC188: 0.0%, 100.0%\n",
      "PC189: 0.0%, 100.0%\n",
      "PC190: 0.0%, 100.0%\n",
      "PC191: 0.0%, 100.0%\n",
      "PC192: 0.0%, 100.0%\n",
      "PC193: 0.0%, 100.0%\n",
      "PC194: 0.0%, 100.0%\n",
      "PC195: 0.0%, 100.0%\n",
      "PC196: 0.0%, 100.0%\n",
      "PC197: 0.0%, 100.0%\n",
      "PC198: 0.0%, 100.0%\n",
      "PCA explained variance report saved to ./03_morphometrics_output_cultivated1st/pca_explained_variance.txt\n",
      "\n",
      "--- Saving PCA model parameters, PC scores, and class labels ---\n",
      "  PCA Components shape: (198, 198)\n",
      "  PCA Mean shape: (198,)\n",
      "  PCA Explained Variance shape: (198,)\n",
      "  PCA Explained Variance Ratio shape: (198,)\n",
      "  Number of PCA components: 198\n",
      "  Original PCA Scores (PCs) shape: (319, 198)\n",
      "  Class Labels (type) length: 319\n",
      "PCA parameters saved to ./03_morphometrics_output_cultivated1st/leaf_pca_model_parameters.h5\n",
      "Original PCA scores, class labels, AND original flattened coordinates saved to ./03_morphometrics_output_cultivated1st/original_pca_scores_and_class_labels.h5\n",
      "\n",
      "--- Creating Morphospace Plot ---\n",
      "Morphospace plot saved to ./03_morphometrics_output_cultivated1st/morphospace_plot.png\n",
      "\n",
      "--- All processing and saving completed ---\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "### LOAD IN MODULES ###\n",
    "#######################\n",
    "\n",
    "import cv2 # to install on mac: pip install opencv-python\n",
    "from scipy.interpolate import interp1d # for interpolating points\n",
    "from sklearn.decomposition import PCA # for principal component analysis\n",
    "from scipy.spatial import procrustes # for Procrustes analysis\n",
    "from scipy.spatial import ConvexHull # for convex hull (not used in provided code yet)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # for LDA (not used yet)\n",
    "from sklearn.metrics import confusion_matrix # for confusion matrix (not used yet)\n",
    "import scipy.stats as stats # for kruskal wallis test (not used yet)\n",
    "import statsmodels.stats.multitest as multitest # multiple test adjustment (not used yet)\n",
    "import itertools # for pairwise combinations (not used yet)\n",
    "from os import listdir # for retrieving files from directory\n",
    "from os.path import isfile, join # for retrieving files from directory\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np # for using arrays\n",
    "import math # for mathematical operations\n",
    "import pandas as pd # for using pandas dataframes\n",
    "import seaborn as sns # for plotting in seaborn\n",
    "from matplotlib.colors import LogNorm # for log scale (not used yet)\n",
    "import phate # for using PHATE (not used yet)\n",
    "import scprep # for using PHATE (not not used yet)\n",
    "import h5py # For saving large arrays and PCA model parameters\n",
    "import pickle # For saving Python objects (not strictly needed now, as leaf_indices is removed)\n",
    "import os # For path operations and directory creation\n",
    "\n",
    "#################\n",
    "### FUNCTIONS ###\n",
    "#################\n",
    "\n",
    "def angle_between(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    define a function to find the angle between 3 points anti-clockwise in degrees, p2 being the vertex\n",
    "    inputs: three angle points, as tuples\n",
    "    output: angle in degrees\n",
    "    \"\"\"\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    x3, y3 = p3\n",
    "    deg1 = (360 + math.degrees(math.atan2(x1 - x2, y1 - y2))) % 360\n",
    "    deg2 = (360 + math.degrees(math.atan2(x3 - x2, y3 - y2))) % 360\n",
    "    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)\n",
    "\n",
    "def rotate_points(xvals, yvals, degrees):\n",
    "    \"\"\"\"\n",
    "    define a function to rotate 2D x and y coordinate points around the origin\n",
    "    inputs: x and y vals (can take pandas dataframe columns) and the degrees (positive, anticlockwise) to rotate\n",
    "    outputs: rotated and y vals\n",
    "    \"\"\"\n",
    "    angle_to_move = 90-degrees\n",
    "    rads = np.deg2rad(angle_to_move)\n",
    "\n",
    "    new_xvals = xvals*np.cos(rads)-yvals*np.sin(rads)\n",
    "    new_yvals = xvals*np.sin(rads)+yvals*np.cos(rads)\n",
    "\n",
    "    return new_xvals, new_yvals\n",
    "\n",
    "def interpolation(x, y, number):\n",
    "    \"\"\"\n",
    "    define a function to return equally spaced, interpolated points for a given polyline\n",
    "    inputs: arrays of x and y values for a polyline, number of points to interpolate\n",
    "    ouputs: interpolated points along the polyline, inclusive of start and end points\n",
    "    \"\"\"\n",
    "    # Check if x or y are empty or have single point, which would cause issues with ediff1d or division by zero.\n",
    "    if len(x) < 2 or len(y) < 2:\n",
    "        # Handle cases where segments are too short\n",
    "        # For a minimum of 2 points, interpolation can work but distance[ -1] might be 0 if points are identical.\n",
    "        # If points are identical, distance will be all zeros.\n",
    "        if np.all(x == x[0]) and np.all(y == y[0]): # all points are identical\n",
    "            # If all points are identical, return the same point 'number' times\n",
    "            return np.full(number, x[0]), np.full(number, y[0])\n",
    "        elif len(x) == 1: # Single point, replicate it\n",
    "             return np.full(number, x[0]), np.full(number, y[0])\n",
    "        else: # Likely a segment with just two points, where the distance calculation might still be problematic if they are identical.\n",
    "            # If `distance[-1]` is zero, it means start and end points are identical.\n",
    "            # This should ideally be caught by len(x) < 2 if it's truly a single point.\n",
    "            # For two points, `ediff1d` works.\n",
    "            pass # Continue with normal interpolation, it should handle 2 points\n",
    "\n",
    "    distance = np.cumsum(np.sqrt( np.ediff1d(x, to_begin=0)**2 + np.ediff1d(y, to_begin=0)**2 ))\n",
    "    \n",
    "    # Check if the total distance is zero (e.g., all points are identical) to prevent division by zero\n",
    "    if distance[-1] == 0:\n",
    "        # If all points are identical, just return 'number' copies of the first point\n",
    "        return np.full(number, x[0]), np.full(number, y[0])\n",
    "\n",
    "    distance = distance/distance[-1]\n",
    "\n",
    "    fx, fy = interp1d( distance, x ), interp1d( distance, y )\n",
    "\n",
    "    alpha = np.linspace(0, 1, number)\n",
    "    x_regular, y_regular = fx(alpha), fy(alpha)\n",
    "\n",
    "    return x_regular, y_regular\n",
    "\n",
    "def euclid_dist(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    define a function to return the euclidean distance between two points\n",
    "    inputs: x and y values of the two points\n",
    "    output: the eulidean distance\n",
    "    \"\"\"\n",
    "    return np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "\n",
    "def poly_area(x,y):\n",
    "    \"\"\"\n",
    "    define a function to calculate the area of a polygon using the shoelace algorithm\n",
    "    inputs: separate numpy arrays of x and y coordinate values\n",
    "    outputs: the area of the polygon\n",
    "    \"\"\"\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def gpa_mean(leaf_arr, landmark_num, dim_num):\n",
    "\n",
    "    \"\"\"\n",
    "    define a function that given an array of landmark data returns the Generalized Procrustes Analysis mean\n",
    "    inputs: a 3 dimensional array of samples by landmarks by coordinate values, number of landmarks, number of dimensions\n",
    "    output: an array of the Generalized Procrustes Analysis mean shape\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ref_ind = 0 # select arbitrary reference index to calculate procrustes distances to\n",
    "    ref_shape = leaf_arr[ref_ind, :, :] # select the reference shape\n",
    "\n",
    "    mean_diff = 10**(-30) # set a distance between means to stop the algorithm\n",
    "\n",
    "    old_mean = ref_shape # for the first comparison between means, set old_mean to an arbitrary reference shape\n",
    "\n",
    "    d = 1000000 # set d initially arbitraily high\n",
    "\n",
    "    while d > mean_diff: # set boolean criterion for Procrustes distance between mean to stop calculations\n",
    "\n",
    "        arr = np.zeros( ((len(leaf_arr)),landmark_num,dim_num) ) # empty 3D array: # samples, landmarks, coord vals\n",
    "\n",
    "        for i in range(len(leaf_arr)): # for each leaf shape\n",
    "\n",
    "            s1, s2, distance = procrustes(old_mean, leaf_arr[i]) # calculate procrustes adjusted shape to ref for current leaf\n",
    "            arr[i] = s2 # store procrustes adjusted shape to array\n",
    "\n",
    "        new_mean = np.mean(arr, axis=(0)) # calculate mean of all shapes adjusted to reference\n",
    "\n",
    "        s1, s2, d = procrustes(old_mean, new_mean) # calculate procrustes distance of new mean to old mean\n",
    "\n",
    "        old_mean = new_mean # set the old_mean to the new_mea before beginning another iteration\n",
    "\n",
    "    return new_mean\n",
    "\n",
    "# --- Configuration and Inputs ---\n",
    "\n",
    "# Input File Paths\n",
    "# --- MODIFIED: Updated METADATA_FILE and IMAGE_DATA_DIR for the new dataset ---\n",
    "METADATA_FILE = \"./01_cultivated1st_landmarks.csv\" # Adjusted path based on your output\n",
    "IMAGE_DATA_DIR = \"./00_cultivated1st_data/\"\n",
    "\n",
    "# Output Directory (will be created if it doesn't exist)\n",
    "# --- MODIFIED: Changed output directory to reflect new dataset ---\n",
    "OUTPUT_BASE_DIR = \"./03_morphometrics_output_cultivated1st/\" # Changed to distinguish output\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True) # Ensure output directory exists\n",
    "\n",
    "# --- Parameters for Preprocessing ---\n",
    "HIGH_RES_INTERPOLATION_POINTS = 10000 # Initial high resolution outline points\n",
    "FINAL_PSEUDO_LANDMARKS_PER_SIDE = 50  # Number of equidistant points on each side (excluding the tip duplicate)\n",
    "                                     # Total pseudo-landmarks will be (FINAL_PSEUDO_LANDMARKS_PER_SIDE * 2) - 1\n",
    "\n",
    "# --- Parameters for Procrustes Analysis ---\n",
    "NUM_LANDMARKS = (FINAL_PSEUDO_LANDMARKS_PER_SIDE * 2) - 1 # Derived from above\n",
    "NUM_DIMENSIONS = 2                                       # For 2D coordinates\n",
    "\n",
    "# --- Parameters for PCA (Full Component Analysis) ---\n",
    "# This PCA is for explained variance analysis and later data augmentation.\n",
    "# It will calculate all possible components up to min(samples, features).\n",
    "\n",
    "# --- Parameters for Morphospace Visualization (2-Component PCA) ---\n",
    "MORPHOSPACE_PLOT_LENGTH = 10 # Plot length in inches\n",
    "MORPHOSPACE_PLOT_WIDTH = 10  # Plot width in inches\n",
    "MORPHOSPACE_PC1_INTERVALS = 20 # Number of PC1 intervals for eigenleaf grid\n",
    "MORPHOSPACE_PC2_INTERVALS = 6  # Number of PC2 intervals for eigenleaf grid\n",
    "MORPHOSPACE_HUE_COLUMN = \"type\" # Column in mdata to color points by for the morphospace plot\n",
    "EIGENLEAF_SCALE = 0.08 # Scaling of the inverse eigenleaves\n",
    "EIGENLEAF_COLOR = \"lightgray\" # Color of inverse eigenleaf\n",
    "EIGENLEAF_ALPHA = 0.5 # Alpha of inverse eigenleaf\n",
    "POINT_SIZE = 80 # Size of data points\n",
    "POINT_LINEWIDTH = 0 # Line width of data points (set to 0 for no edges)\n",
    "POINT_ALPHA = 0.6 # Alpha of the data points\n",
    "AXIS_LABEL_FONTSIZE = 12 # Font size of the x and y axis titles\n",
    "AXIS_TICK_FONTSIZE = 8 # Font size of the axis ticks\n",
    "FACE_COLOR = \"white\" # Color of the plot background\n",
    "GRID_ALPHA = 0.5 # Alpha of the grid\n",
    "\n",
    "# --- Parameters for Output Files ---\n",
    "GPA_MEAN_SHAPE_PLOT_FILENAME = \"gpa_mean_shape.png\"\n",
    "PCA_EXPLAINED_VARIANCE_REPORT_FILENAME = \"pca_explained_variance.txt\"\n",
    "MORPHOSPACE_PLOT_FILENAME = \"morphospace_plot.png\"\n",
    "\n",
    "# Specific filenames for saving PCA components, scores, and labels (using h5py)\n",
    "PCA_PARAMS_H5_FILENAME = \"leaf_pca_model_parameters.h5\"\n",
    "ORIGINAL_PCA_SCORES_AND_LABELS_H5_FILENAME = \"original_pca_scores_and_class_labels.h5\"\n",
    "CLASS_LABEL_COLUMN_FOR_SAVING = \"type\" # The column from mdata to use for class labels (e.g., 'type', 'cultivar', etc.)\n",
    "\n",
    "# E.g., FIGURE_DPI = 300 # Default DPI for saved figures\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Saving outputs to directory: {OUTPUT_BASE_DIR}\")\n",
    "\n",
    "########################\n",
    "### READ IN METADATA ###\n",
    "########################\n",
    "\n",
    "# --- MODIFIED: Simplified metadata loading as the new CSV is pre-formatted ---\n",
    "mdata = pd.read_csv(METADATA_FILE) # read in csv\n",
    "\n",
    "# The 'file', 'type', 'base_x', 'base_y', 'tip_x', 'tip_y' columns are already present\n",
    "# and correctly formatted in the new '00_cultivated1st_landmarks.csv'.\n",
    "# Therefore, the following steps are no longer needed:\n",
    "# Step 1: Sort the original DataFrame to ensure correct base/tip pairing\n",
    "# mdata_sorted = mdata.sort_values(by=['Label', 'index'])\n",
    "# Step 2: Group by 'Label' and aggregate to create 'base_x', 'base_y', 'tip_x', 'tip_y'\n",
    "# new_df = mdata_sorted.groupby('Label').agg(\n",
    "#     base_x=('X', lambda x: x.iloc[0]),\n",
    "#     base_y=('Y', lambda x: x.iloc[0]),\n",
    "#     tip_x=('X', lambda x: x.iloc[1]),\n",
    "#     tip_y=('Y', lambda x: x.iloc[1])\n",
    "# ).reset_index()\n",
    "# Step 3: Rename the 'Label' column to 'file'\n",
    "# mdata = new_df.rename(columns={'Label': 'file'})\n",
    "# Step 4: Extract the 'type' (class name) from the 'file' column\n",
    "# This step is also no longer needed as 'type' column is already present\n",
    "# mdata['type'] = mdata['file'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "print(f\"Metadata loaded from: {METADATA_FILE}\")\n",
    "print(\"First 5 rows of loaded metadata:\")\n",
    "print(mdata.head())\n",
    "\n",
    "\n",
    "#######################################\n",
    "### MAKE A LIST OF IMAGE FILE NAMES ###\n",
    "#######################################\n",
    "\n",
    "# It's good practice to ensure all files listed in mdata exist in the directory.\n",
    "# Also, ensure file_names list corresponds to files in mdata to avoid mismatches.\n",
    "# The previous approach used listdir, which might include files not in mdata or vice-versa.\n",
    "# Let's ensure we only process files explicitly mentioned in mdata.\n",
    "# We will use the 'file' column from the loaded mdata.\n",
    "\n",
    "file_names = mdata['file'].tolist() # Get filenames directly from the metadata DataFrame\n",
    "file_names.sort() # Ensure consistent order\n",
    "\n",
    "# --- REMOVED: No longer need to remove .DS_Store from listdir output as we're using mdata's filenames ---\n",
    "# if '.DS_Store' in file_names:\n",
    "#     file_names.remove('.DS_Store')\n",
    "\n",
    "print(f\"Found {len(file_names)} image files to process from metadata.\")\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "### INTERPOLATE POINTS CREATING PSEUDO-LANDMARKS AND PRE-PROCESS ###\n",
    "#####################################################################\n",
    "\n",
    "print(\"\\n--- Preprocessing Images and Interpolating Pseudo-Landmarks ---\")\n",
    "# an array to store pseudo-landmarks\n",
    "cult_cm_arr = np.zeros((len(mdata), NUM_LANDMARKS, NUM_DIMENSIONS))\n",
    "\n",
    "# for each leaf . . .\n",
    "# We iterate using mdata.iterrows() for robust access to row data (including index)\n",
    "for lf_idx, row in mdata.iterrows(): # Use lf_idx for iteration, row for data access\n",
    "\n",
    "    curr_image_filename = row[\"file\"] # Select the current image filename from the row\n",
    "    # print(f\"Processing leaf {lf_idx+1}/{len(mdata)}: {curr_image_filename}\") # Optional: progress indicator\n",
    "\n",
    "    img_path = os.path.join(IMAGE_DATA_DIR, curr_image_filename)\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"Warning: Image file not found at {img_path}. Skipping.\")\n",
    "        # You might want to handle this more robustly, e.g., by logging or removing from mdata\n",
    "        # For now, we'll store zeros for skipped entries in cult_cm_arr, which might affect GPA mean later.\n",
    "        # A better approach for robust pipelines: filter mdata BEFORE this loop.\n",
    "        # For this fix, let's just ensure no error if a file is missing.\n",
    "        continue # Skip to the next iteration\n",
    "\n",
    "    img = cv2.bitwise_not(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(img,\n",
    "        cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    x_conts = []\n",
    "    y_conts = []\n",
    "    areas_conts = []\n",
    "    for c in contours:\n",
    "        x_vals = []\n",
    "        y_vals = []\n",
    "        for i in range(len(c)):\n",
    "            x_vals.append(c[i][0][0])\n",
    "            y_vals.append(c[i][0][1])\n",
    "        area = (max(x_vals) - min(x_vals))*(max(y_vals) - min(y_vals))\n",
    "        x_conts.append(x_vals)\n",
    "        y_conts.append(y_vals)\n",
    "        areas_conts.append(area)\n",
    "    \n",
    "    # Handle cases where no contours are found or contours are too small\n",
    "    if not areas_conts:\n",
    "        print(f\"Warning: No contours found for image {curr_image_filename}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    area_inds = np.flip(np.argsort(areas_conts))\n",
    "    sorted_x_conts = np.array(x_conts, dtype=object)[area_inds][0:]\n",
    "    sorted_y_conts = np.array(y_conts, dtype=object)[area_inds][0:]\n",
    "\n",
    "    high_res_x, high_res_y = interpolation(np.array(sorted_x_conts[0], dtype=np.float32),\n",
    "                                            np.array(sorted_y_conts[0], dtype=np.float32), HIGH_RES_INTERPOLATION_POINTS)\n",
    "\n",
    "    base_pt = np.array((row[\"base_x\"], row[\"base_y\"]))\n",
    "    tip_pt = np.array((row[\"tip_x\"], row[\"tip_y\"]))\n",
    "\n",
    "    base_dists = []\n",
    "    tip_dists = []\n",
    "\n",
    "    for pt in range(len(high_res_x)):\n",
    "        ed_base = euclid_dist(base_pt[0], base_pt[1], high_res_x[pt], high_res_y[pt])\n",
    "        ed_tip = euclid_dist(tip_pt[0], tip_pt[1], high_res_x[pt], high_res_y[pt])\n",
    "        base_dists.append(ed_base)\n",
    "        tip_dists.append(ed_tip)\n",
    "\n",
    "    base_ind = np.argmin(base_dists)\n",
    "    tip_ind = np.argmin(tip_dists)\n",
    "\n",
    "    # Reorder the contour to start at the base point\n",
    "    high_res_x = np.concatenate((high_res_x[base_ind:], high_res_x[:base_ind]))\n",
    "    high_res_y = np.concatenate((high_res_y[base_ind:], high_res_y[:base_ind]))\n",
    "    \n",
    "    # Now that the array starts at the base point, the base_ind is effectively 0.\n",
    "    # We need to find the new index of the tip point in this reordered array.\n",
    "    # Since we reordered by shifting, the new tip_ind can be calculated by adjusting the original.\n",
    "    # If original tip_ind was before original base_ind, new index = original_tip_ind + (total_points - original_base_ind)\n",
    "    # If original tip_ind was after original base_ind, new index = original_tip_ind - original_base_ind\n",
    "    \n",
    "    # A more robust way to find tip_ind_new is to search for the tip_pt in the reordered high_res_x/y\n",
    "    # Or, as previously, use the smallest distance from the tip_pt in the reordered array.\n",
    "    # The previous `tip_ind_new = np.argmin(tip_dists_after_rot)` from the previous response was trying to achieve this.\n",
    "    # Let's simplify and directly find the tip_ind in the new shifted array.\n",
    "    \n",
    "    # Find the index of the tip point in the newly rotated array\n",
    "    # This is safer than `tip_ind = tip_ind-base_ind` after the concatenation.\n",
    "    new_tip_dists = []\n",
    "    for pt_idx in range(len(high_res_x)):\n",
    "        new_tip_dists.append(euclid_dist(tip_pt[0], tip_pt[1], high_res_x[pt_idx], high_res_y[pt_idx]))\n",
    "    tip_ind_new = np.argmin(new_tip_dists)\n",
    "\n",
    "    lf_contour = np.column_stack((high_res_x, high_res_y))\n",
    "\n",
    "    # Define the left and right segments correctly, ensuring they loop properly and share the base/tip points\n",
    "    # Left segment: from base (index 0) to tip (tip_ind_new)\n",
    "    left_segment = lf_contour[0:tip_ind_new+1, :] # Includes base and tip\n",
    "\n",
    "    # Right segment: from tip (tip_ind_new) back to base (index 0), ensuring the base point is included at the end\n",
    "    # This means the right_segment starts at tip_ind_new, goes to the end, and then wraps around to the beginning (index 0).\n",
    "    right_segment = np.concatenate((lf_contour[tip_ind_new:, :], lf_contour[0:1, :]), axis=0)\n",
    "    \n",
    "    # Ensure segments have at least 2 points for interpolation to work reliably\n",
    "    if len(left_segment) < 2 or len(right_segment) < 2:\n",
    "        print(f\"Warning: Segments for image {curr_image_filename} are too short for interpolation. Skipping.\")\n",
    "        continue # Skip this image\n",
    "\n",
    "    left_inter_x, left_inter_y = interpolation(left_segment[:,0], left_segment[:,1], FINAL_PSEUDO_LANDMARKS_PER_SIDE)\n",
    "    right_inter_x, right_inter_y = interpolation(right_segment[:,0], right_segment[:,1], FINAL_PSEUDO_LANDMARKS_PER_SIDE)\n",
    "\n",
    "\n",
    "    left_inter_x = np.delete(left_inter_x, -1) # Remove last point of left side (which is the tip, to avoid duplication)\n",
    "    left_inter_y = np.delete(left_inter_y, -1) # Remove last point of left side (which is the tip, to avoid duplication)\n",
    "\n",
    "    lf_pts_left = np.column_stack((left_inter_x, left_inter_y))\n",
    "    lf_pts_right = np.column_stack((right_inter_x, right_inter_y))\n",
    "    lf_pts = np.row_stack((lf_pts_left, lf_pts_right))\n",
    "\n",
    "    # Ensure the total number of landmarks is correct\n",
    "    if lf_pts.shape[0] != NUM_LANDMARKS:\n",
    "        print(f\"Warning: Leaf {curr_image_filename} generated {lf_pts.shape[0]} landmarks, expected {NUM_LANDMARKS}. Check interpolation logic.\")\n",
    "        # This warning is important if the `FINAL_PSEUDO_LANDMARKS_PER_SIDE` and total `NUM_LANDMARKS` calculation results in an off-by-one error for some cases.\n",
    "        # The current calculation is (N_side * 2) - 1 because the tip is counted once, and the base is effectively the start/end point.\n",
    "        # If FINAL_PSEUDO_LANDMARKS_PER_SIDE includes the endpoint (as interpolation does), then\n",
    "        # left_inter has N points (base to tip)\n",
    "        # right_inter has N points (tip to base)\n",
    "        # if we remove tip from left, left has N-1 points.\n",
    "        # if we assume right starts at tip and ends at base, it has N points.\n",
    "        # Total = (N-1) + N = 2N-1. This seems correct.\n",
    "\n",
    "    tip_point = lf_pts[FINAL_PSEUDO_LANDMARKS_PER_SIDE-1,:] # This should be the tip (last point of left_inter_x before deletion)\n",
    "    base_point = lf_pts[0,:] # This should be the base\n",
    "\n",
    "    ang = angle_between(tip_point, base_point, (base_point[0]+1,base_point[1]) )\n",
    "\n",
    "    rot_x, rot_y = rotate_points(lf_pts[:,0], lf_pts[:,1], ang)\n",
    "    rot_pts = np.column_stack((rot_x, rot_y))\n",
    "\n",
    "    # Store the processed points in the array based on its original index in mdata\n",
    "    cult_cm_arr[lf_idx,:,:] = rot_pts\n",
    "\n",
    "##########################\n",
    "### CALCULATE GPA MEAN ###\n",
    "##########################\n",
    "\n",
    "print(\"\\n--- Calculating GPA Mean ---\")\n",
    "mean_shape = gpa_mean(cult_cm_arr, NUM_LANDMARKS, NUM_DIMENSIONS)\n",
    "\n",
    "################################\n",
    "### ALIGN LEAVES TO GPA MEAN ###\n",
    "################################\n",
    "\n",
    "print(\"--- Aligning Leaves to GPA Mean ---\")\n",
    "proc_arr = np.zeros(np.shape(cult_cm_arr))\n",
    "\n",
    "for i in range(len(cult_cm_arr)):\n",
    "    s1, s2, distance = procrustes(mean_shape, cult_cm_arr[i, :, :])\n",
    "    proc_arr[i] = s2\n",
    "\n",
    "#### VISUALIZE GPA ALIGNED SHAPES AND MEAN\n",
    "print(\"--- Visualizing GPA Aligned Shapes ---\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(len(proc_arr)):\n",
    "    plt.plot(proc_arr[i, :, 0], proc_arr[i, :, 1], c=\"k\", alpha=0.08)\n",
    "\n",
    "plt.plot(np.mean(proc_arr, axis=0)[:, 0], np.mean(proc_arr, axis=0)[:, 1], c=\"magenta\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Procrustes Aligned Leaf Shapes and GPA Mean\")\n",
    "\n",
    "plt.savefig(os.path.join(OUTPUT_BASE_DIR, GPA_MEAN_SHAPE_PLOT_FILENAME))\n",
    "plt.close()\n",
    "print(f\"GPA mean shape plot saved to {os.path.join(OUTPUT_BASE_DIR, GPA_MEAN_SHAPE_PLOT_FILENAME)}\")\n",
    "\n",
    "\n",
    "#################################################\n",
    "### FIRST, CALCULATE PERCENT VARIANCE ALL PCs ###\n",
    "#################################################\n",
    "\n",
    "print(\"\\n--- Performing Full PCA and Generating Explained Variance Report ---\")\n",
    "\n",
    "# use the reshape function to flatten to 2D\n",
    "flat_arr = proc_arr.reshape(np.shape(proc_arr)[0],\n",
    "                             np.shape(proc_arr)[1] * np.shape(proc_arr)[2])\n",
    "\n",
    "# Determine the maximum number of principal components possible: min(n_samples, n_features)\n",
    "max_pc_components = min(flat_arr.shape[0], flat_arr.shape[1])\n",
    "\n",
    "# Initialize PCA to calculate all possible PCs for full variance analysis\n",
    "pca = PCA(n_components=max_pc_components)\n",
    "PCs = pca.fit_transform(flat_arr) # fit a PCA for all data\n",
    "\n",
    "# Generate and save explained variance report\n",
    "pca_explained_variance_filepath = os.path.join(OUTPUT_BASE_DIR, PCA_EXPLAINED_VARIANCE_REPORT_FILENAME)\n",
    "with open(pca_explained_variance_filepath, 'w') as f:\n",
    "    f.write(\"PCA Explained Variance Report:\\n\")\n",
    "    f.write(f\"Total Samples: {flat_arr.shape[0]}\\n\")\n",
    "    f.write(f\"Total Features (landmarks * dimensions): {flat_arr.shape[1]}\\n\")\n",
    "    f.write(f\"Number of PCs Calculated: {pca.n_components_}\\n\\n\")\n",
    "\n",
    "    f.write(\"PC: var, overall\\n\")\n",
    "    for i in range(len(pca.explained_variance_ratio_)):\n",
    "        pc_variance = round(pca.explained_variance_ratio_[i] * 100, 2)\n",
    "        cumulative_variance = round(pca.explained_variance_ratio_.cumsum()[i] * 100, 2)\n",
    "        line = f\"PC{i+1}: {pc_variance}%, {cumulative_variance}%\\n\"\n",
    "        print(line.strip()) # Also print to console\n",
    "        f.write(line)\n",
    "print(f\"PCA explained variance report saved to {pca_explained_variance_filepath}\")\n",
    "\n",
    "# --- Save PCA Model Parameters, PC Scores, and Class Labels ---\n",
    "print(\"\\n--- Saving PCA model parameters, PC scores, and class labels ---\")\n",
    "\n",
    "# 1. Extract information from the PCA model and original data\n",
    "pca_components = pca.components_\n",
    "pca_mean = pca.mean_\n",
    "pca_explained_variance = pca.explained_variance_\n",
    "pca_explained_variance_ratio = pca.explained_variance_ratio_\n",
    "n_pca_components = pca.n_components_\n",
    "\n",
    "print(f\"  PCA Components shape: {pca_components.shape}\")\n",
    "print(f\"  PCA Mean shape: {pca_mean.shape}\")\n",
    "print(f\"  PCA Explained Variance shape: {pca_explained_variance.shape}\")\n",
    "print(f\"  PCA Explained Variance Ratio shape: {pca_explained_variance_ratio.shape}\")\n",
    "print(f\"  Number of PCA components: {n_pca_components}\")\n",
    "print(f\"  Original PCA Scores (PCs) shape: {PCs.shape}\")\n",
    "print(f\"  Class Labels ({CLASS_LABEL_COLUMN_FOR_SAVING}) length: {len(mdata[CLASS_LABEL_COLUMN_FOR_SAVING])}\")\n",
    "\n",
    "# 2. Save the PCA model parameters to an HDF5 file\n",
    "pca_params_filepath = os.path.join(OUTPUT_BASE_DIR, PCA_PARAMS_H5_FILENAME)\n",
    "with h5py.File(pca_params_filepath, 'w') as f:\n",
    "    f.create_dataset('components', data=pca_components, compression=\"gzip\")\n",
    "    f.create_dataset('mean', data=pca_mean, compression=\"gzip\")\n",
    "    f.create_dataset('explained_variance', data=pca_explained_variance, compression=\"gzip\")\n",
    "    f.create_dataset('explained_variance_ratio', data=pca_explained_variance_ratio, compression=\"gzip\")\n",
    "    f.attrs['n_components'] = n_pca_components\n",
    "print(f\"PCA parameters saved to {pca_params_filepath}\")\n",
    "\n",
    "# 3. Save original PCA scores (PCs) and class labels to an HDF5 file\n",
    "pca_scores_labels_filepath = os.path.join(OUTPUT_BASE_DIR, ORIGINAL_PCA_SCORES_AND_LABELS_H5_FILENAME)\n",
    "with h5py.File(pca_scores_labels_filepath, 'w') as f:\n",
    "    f.create_dataset('pca_scores', data=PCs, compression=\"gzip\")\n",
    "    # Convert labels to a numpy array of byte strings for HDF5 compatibility\n",
    "    f.create_dataset('class_labels', data=np.array(mdata[CLASS_LABEL_COLUMN_FOR_SAVING]).astype('S'), compression=\"gzip\")\n",
    "    # --- ADDED: Save the original flattened coordinates ---\n",
    "    f.create_dataset('original_flattened_coords', data=flat_arr, compression=\"gzip\")\n",
    "print(f\"Original PCA scores, class labels, AND original flattened coordinates saved to {pca_scores_labels_filepath}\")\n",
    "\n",
    "\n",
    "##########################\n",
    "### CREATE MORPHOSPACE ###\n",
    "##########################\n",
    "\n",
    "print(\"\\n--- Creating Morphospace Plot ---\")\n",
    "\n",
    "# The flat_arr is already prepared from the previous full PCA step.\n",
    "# flat_arr = proc_arr.reshape(np.shape(proc_arr)[0],\n",
    "#                              np.shape(proc_arr)[1] * np.shape(proc_arr)[2])\n",
    "\n",
    "# Perform PCA specifically for morphospace visualization (2 components)\n",
    "morphospace_pca = PCA(n_components=2)\n",
    "morphospace_PCs = morphospace_pca.fit_transform(flat_arr)\n",
    "\n",
    "# Add the 2-component PCA results to the mdata DataFrame\n",
    "mdata[\"PC1\"] = morphospace_PCs[:, 0]\n",
    "mdata[\"PC2\"] = morphospace_PCs[:, 1]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(MORPHOSPACE_PLOT_LENGTH, MORPHOSPACE_PLOT_WIDTH))\n",
    "plt.gca().set_facecolor(FACE_COLOR)\n",
    "plt.gca().set_axisbelow(True)\n",
    "\n",
    "# Create PC intervals for plotting inverse eigenleaves\n",
    "PC1_vals = np.linspace(np.min(PCs[:, 0]), np.max(PCs[:, 0]), MORPHOSPACE_PC1_INTERVALS)\n",
    "PC2_vals = np.linspace(np.min(PCs[:, 1]), np.max(PCs[:, 1]), MORPHOSPACE_PC2_INTERVALS)\n",
    "\n",
    "# Plot inverse eigenleaves (the background grid shapes)\n",
    "for i in PC1_vals:\n",
    "    for j in PC2_vals:\n",
    "        inv_leaf = morphospace_pca.inverse_transform(np.array([i, j]))\n",
    "        inv_leaf_coords = inv_leaf.reshape(NUM_LANDMARKS, NUM_DIMENSIONS) # Reshape back to 2D points\n",
    "\n",
    "        inv_x = inv_leaf_coords[:, 0]\n",
    "        inv_y = inv_leaf_coords[:, 1]\n",
    "\n",
    "        plt.fill(inv_x * EIGENLEAF_SCALE + i, inv_y * EIGENLEAF_SCALE + j,\n",
    "                 c=EIGENLEAF_COLOR, alpha=EIGENLEAF_ALPHA)\n",
    "\n",
    "# Plot the data points on top of the morphospace\n",
    "sns.scatterplot(data=mdata, x=\"PC1\", y=\"PC2\", hue=MORPHOSPACE_HUE_COLUMN,\n",
    "                s=POINT_SIZE, linewidth=POINT_LINEWIDTH, alpha=POINT_ALPHA)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9})\n",
    "\n",
    "# Customize axis labels using explained variance from the FULL PCA\n",
    "# Ensure `pca.explained_variance_ratio_` is accessible from the full PCA.\n",
    "xlab = f\"PC1, {round(pca.explained_variance_ratio_[0] * 100, 1)}%\"\n",
    "ylab = f\"PC2, {round(pca.explained_variance_ratio_[1] * 100, 1)}%\"\n",
    "plt.xlabel(xlab, fontsize=AXIS_LABEL_FONTSIZE)\n",
    "plt.ylabel(ylab, fontsize=AXIS_LABEL_FONTSIZE)\n",
    "plt.xticks(fontsize=AXIS_TICK_FONTSIZE)\n",
    "plt.yticks(fontsize=AXIS_TICK_FONTSIZE)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(OUTPUT_BASE_DIR, MORPHOSPACE_PLOT_FILENAME), bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Morphospace plot saved to {os.path.join(OUTPUT_BASE_DIR, MORPHOSPACE_PLOT_FILENAME)}\")\n",
    "\n",
    "print(\"\\n--- All processing and saving completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c63db-f382-43a2-b66f-a5ea4b320d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
