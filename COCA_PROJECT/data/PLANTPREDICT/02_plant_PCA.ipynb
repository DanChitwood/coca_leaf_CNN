{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987f97f8-c128-489a-9462-99e44ac048e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Starting Analysis for PLANT_PREDICT Dataset ==========\n",
      "Saving outputs to directory: ./03_morphometrics_output_plant_predict/\n",
      "Metadata loaded from: ./01_cultivated2nd_landmarks.csv\n",
      "First 5 rows of loaded metadata:\n",
      "          file            species  px_cm  base_x  base_y   tip_x   tip_y  \\\n",
      "0  AMA1A_a.tif  Erythroxylym coca   28.4  271.00  195.25  127.50  189.00   \n",
      "1  AMA1A_b.tif  Erythroxylym coca   28.4  275.00  202.75  125.00  193.25   \n",
      "2  AMA1A_c.tif  Erythroxylym coca   28.4  271.25  202.50  127.75  194.00   \n",
      "3  AMA1A_d.tif  Erythroxylym coca   28.4  281.00  208.00  118.00  193.00   \n",
      "4  AMA1A_e.tif  Erythroxylym coca   28.4  284.25  210.25  115.50  200.00   \n",
      "\n",
      "  variety plant leaf full_name  type plantID  \n",
      "0     AMA    1A    a   amazona  coca  AMA_1A  \n",
      "1     AMA    1A    b   amazona  coca  AMA_1A  \n",
      "2     AMA    1A    c   amazona  coca  AMA_1A  \n",
      "3     AMA    1A    d   amazona  coca  AMA_1A  \n",
      "4     AMA    1A    e   amazona  coca  AMA_1A  \n",
      "\n",
      "--- Plant ID Class Information ---\n",
      "\n",
      "Number of different plantID classes for each full_name (variety):\n",
      "full_name\n",
      "BON                       15\n",
      "DES                       21\n",
      "POM                       15\n",
      "amazona                   12\n",
      "boliviana blanca          15\n",
      "boliviana roja            15\n",
      "chiparra                  30\n",
      "chirosa                   15\n",
      "crespa                    15\n",
      "dulce                     15\n",
      "gigante                   15\n",
      "guayaba roja              15\n",
      "patirroja                 15\n",
      "peruana roja              15\n",
      "tingo maria               15\n",
      "tingo pajarita             9\n",
      "tingo pajarita caucana    15\n",
      "tingo peruana             15\n",
      "trujillense caucana       15\n",
      "\n",
      "Total number of unique plantID classes: 297\n",
      "\n",
      "Counts of each plantID class:\n",
      "plantID\n",
      "AMA_1A     14\n",
      "AMA_1B     10\n",
      "AMA_1M     11\n",
      "AMA_2A     10\n",
      "AMA_2B     11\n",
      "AMA_2M     11\n",
      "AMA_3A     10\n",
      "AMA_3B      9\n",
      "AMA_3M     10\n",
      "AMA_4A     11\n",
      "AMA_4B     11\n",
      "AMA_4M     10\n",
      "BOB_1A     12\n",
      "BOB_1B     10\n",
      "BOB_1M     10\n",
      "BOB_2A     13\n",
      "BOB_2B     10\n",
      "BOB_2M     10\n",
      "BOB_3A     11\n",
      "BOB_3B      8\n",
      "BOB_3M     12\n",
      "BOB_4A     10\n",
      "BOB_4B     12\n",
      "BOB_4M     13\n",
      "BOB_5A     10\n",
      "BOB_5B     10\n",
      "BOB_5M     10\n",
      "BON_1A     10\n",
      "BON_1B     10\n",
      "BON_1M     10\n",
      "BON_2A     10\n",
      "BON_2B     10\n",
      "BON_2M     12\n",
      "BON_3A      9\n",
      "BON_3B     10\n",
      "BON_3M     10\n",
      "BON_4A     11\n",
      "BON_4B     10\n",
      "BON_4M     10\n",
      "BON_5A     13\n",
      "BON_5B     10\n",
      "BON_5M     10\n",
      "BRO_1A     11\n",
      "BRO_1B     12\n",
      "BRO_1M     10\n",
      "BRO_2A     10\n",
      "BRO_2B     10\n",
      "BRO_2M     11\n",
      "BRO_3A     10\n",
      "BRO_3B     12\n",
      "BRO_3M     10\n",
      "BRO_4      12\n",
      "BRO_4B     10\n",
      "BRO_4M     10\n",
      "BRO_5A     10\n",
      "BRO_5B     12\n",
      "BRO_5M     10\n",
      "CHA_10A    16\n",
      "CHA_10B    10\n",
      "CHA_10M    12\n",
      "CHA_1A      9\n",
      "CHA_1B     10\n",
      "CHA_1M     13\n",
      "CHA_2A     10\n",
      "CHA_2B     12\n",
      "CHA_2M     14\n",
      "CHA_3A     11\n",
      "CHA_3B     10\n",
      "CHA_3M     10\n",
      "CHA_4A     10\n",
      "CHA_4B     12\n",
      "CHA_4M     10\n",
      "CHA_5A     11\n",
      "CHA_5B     10\n",
      "CHA_5M     10\n",
      "CHA_6A     10\n",
      "CHA_6B     12\n",
      "CHA_6M     14\n",
      "CHA_7A     14\n",
      "CHA_7B      9\n",
      "CHA_7M     16\n",
      "CHA_8A     10\n",
      "CHA_8B     10\n",
      "CHA_8M     12\n",
      "CHA_9A     12\n",
      "CHA_9B     14\n",
      "CHA_9M      9\n",
      "CHI_1A     12\n",
      "CHI_1B     10\n",
      "CHI_1M     15\n",
      "CHI_2A     10\n",
      "CHI_2B     11\n",
      "CHI_2M     10\n",
      "CHI_3A     10\n",
      "CHI_3B     10\n",
      "CHI_3M     10\n",
      "CHI_4A     12\n",
      "CHI_4B     10\n",
      "CHI_4M     10\n",
      "CHI_5A     12\n",
      "CHI_5B     10\n",
      "CHI_5M     10\n",
      "CRE_1A     17\n",
      "CRE_1B     11\n",
      "CRE_1M     12\n",
      "CRE_2A     11\n",
      "CRE_2B     10\n",
      "CRE_2M     11\n",
      "CRE_3A     16\n",
      "CRE_3B     12\n",
      "CRE_3M     11\n",
      "CRE_4A     12\n",
      "CRE_4B     11\n",
      "CRE_4M     14\n",
      "CRE_5A     14\n",
      "CRE_5B     12\n",
      "CRE_5M     12\n",
      "DES_1A     12\n",
      "DES_1B     14\n",
      "DES_1M     10\n",
      "DES_2A     11\n",
      "DES_2B     13\n",
      "DES_2M     10\n",
      "DES_C1A    10\n",
      "DES_C1B    10\n",
      "DES_C1M    10\n",
      "DES_C2A    10\n",
      "DES_C2B    10\n",
      "DES_C2M    10\n",
      "DES_C3A    10\n",
      "DES_C3B    10\n",
      "DES_C3M    11\n",
      "DES_C4A    12\n",
      "DES_C4B    13\n",
      "DES_C4M    10\n",
      "DES_C5A    13\n",
      "DES_C5B    10\n",
      "DES_C5M    10\n",
      "DUL_1A     10\n",
      "DUL_1B     14\n",
      "DUL_1M     11\n",
      "DUL_2A      9\n",
      "DUL_2B     10\n",
      "DUL_2M     10\n",
      "DUL_3A     10\n",
      "DUL_3B     10\n",
      "DUL_3M     10\n",
      "DUL_4A     10\n",
      "DUL_4B     10\n",
      "DUL_4M     10\n",
      "DUL_5A     10\n",
      "DUL_5B     13\n",
      "DUL_5M     10\n",
      "GIG_1A     13\n",
      "GIG_1B     10\n",
      "GIG_1M     10\n",
      "GIG_2A     10\n",
      "GIG_2B     10\n",
      "GIG_2M     10\n",
      "GIG_3A     10\n",
      "GIG_3B     10\n",
      "GIG_3M     10\n",
      "GIG_4A     13\n",
      "GIG_4B     10\n",
      "GIG_4M     10\n",
      "GIG_5A     10\n",
      "GIG_5B     10\n",
      "GIG_5M     10\n",
      "GUR_1A     10\n",
      "GUR_1B     10\n",
      "GUR_1M     11\n",
      "GUR_2A     10\n",
      "GUR_2B     10\n",
      "GUR_2M     10\n",
      "GUR_3A     10\n",
      "GUR_3B     10\n",
      "GUR_3M     10\n",
      "GUR_4A     10\n",
      "GUR_4B     10\n",
      "GUR_4M     10\n",
      "GUR_5A     10\n",
      "GUR_5B     10\n",
      "GUR_5M     11\n",
      "PAT_1A     13\n",
      "PAT_1B     14\n",
      "PAT_1M     13\n",
      "PAT_2A     10\n",
      "PAT_2B     10\n",
      "PAT_2M     10\n",
      "PAT_3A     10\n",
      "PAT_3B     10\n",
      "PAT_3M     12\n",
      "PAT_4A     10\n",
      "PAT_4B     10\n",
      "PAT_4M     11\n",
      "PAT_5A     11\n",
      "PAT_5B     10\n",
      "PAT_5M     10\n",
      "PER_1A     16\n",
      "PER_1B     10\n",
      "PER_1M     12\n",
      "PER_2A     10\n",
      "PER_2B     11\n",
      "PER_2M     10\n",
      "PER_3A     10\n",
      "PER_3B     14\n",
      "PER_3M     10\n",
      "PER_4A     11\n",
      "PER_4B     11\n",
      "PER_4M     10\n",
      "PER_5A     10\n",
      "PER_5B     10\n",
      "PER_5M     10\n",
      "POM_1A     10\n",
      "POM_1B     10\n",
      "POM_1M     12\n",
      "POM_2A     14\n",
      "POM_2B     10\n",
      "POM_2M     10\n",
      "POM_3A     11\n",
      "POM_3B     11\n",
      "POM_3M     10\n",
      "POM_4A     10\n",
      "POM_4B     10\n",
      "POM_4M     13\n",
      "POM_5A     11\n",
      "POM_5B     10\n",
      "POM_5M     12\n",
      "TIP_1A     18\n",
      "TIP_1B     10\n",
      "TIP_1M     12\n",
      "TIP_2A     10\n",
      "TIP_2B     12\n",
      "TIP_2M     10\n",
      "TIP_3A     12\n",
      "TIP_3B     10\n",
      "TIP_3M     10\n",
      "TMA_1A     13\n",
      "TMA_1B     10\n",
      "TMA_1M     10\n",
      "TMA_2A     10\n",
      "TMA_2B     11\n",
      "TMA_2M     10\n",
      "TMA_3A     12\n",
      "TMA_3B     10\n",
      "TMA_3M     10\n",
      "TMA_4A     10\n",
      "TMA_4B     11\n",
      "TMA_4M     11\n",
      "TMA_5A     10\n",
      "TMA_5B     10\n",
      "TMA_5M     10\n",
      "TPC_1A     14\n",
      "TPC_1B     15\n",
      "TPC_1M     13\n",
      "TPC_2A     15\n",
      "TPC_2B     11\n",
      "TPC_2M     15\n",
      "TPC_3A     13\n",
      "TPC_3B     14\n",
      "TPC_3M     13\n",
      "TPC_4A     13\n",
      "TPC_4B     11\n",
      "TPC_4M     13\n",
      "TPC_5A     17\n",
      "TPC_5B     11\n",
      "TPC_5M     12\n",
      "TPE_1A     10\n",
      "TPE_1B     10\n",
      "TPE_1M     10\n",
      "TPE_2A     10\n",
      "TPE_2B     11\n",
      "TPE_2M     10\n",
      "TPE_3A     11\n",
      "TPE_3B     10\n",
      "TPE_3M     10\n",
      "TPE_4A     12\n",
      "TPE_4B     10\n",
      "TPE_4M     10\n",
      "TPE_5A     10\n",
      "TPE_5B     10\n",
      "TPE_5M     10\n",
      "TRU_1A     10\n",
      "TRU_1B     10\n",
      "TRU_1M     10\n",
      "TRU_2A     11\n",
      "TRU_2B     11\n",
      "TRU_2M     10\n",
      "TRU_3A     10\n",
      "TRU_3B     11\n",
      "TRU_3M     10\n",
      "TRU_4A     10\n",
      "TRU_4B     10\n",
      "TRU_4M     10\n",
      "TRU_5A     12\n",
      "TRU_5B     10\n",
      "TRU_5M     11\n",
      "-----------------------------------\n",
      "Found 3253 image files to process from metadata.\n",
      "\n",
      "--- Preprocessing Images and Interpolating Pseudo-Landmarks ---\n",
      "Error processing image BRO1B_k.tif: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      ". Skipping.\n",
      "Error processing image CHA7A_j.tif: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      ". Skipping.\n",
      "Error processing image CHA8M_i.tif: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      ". Skipping.\n",
      "Error processing image DES1B_l.tif: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      ". Skipping.\n",
      "Error processing image DUL1B_g.tif: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      ". Skipping.\n",
      "Error processing image DUL5B_h.tif: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      ". Skipping.\n",
      "Error processing image DUL5B_k.tif: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      ". Skipping.\n",
      "--- Calculating GPA Mean ---\n",
      "--- Aligning Leaves to GPA Mean ---\n",
      "--- Visualizing GPA Aligned Shapes ---\n",
      "GPA mean shape plot saved to ./03_morphometrics_output_plant_predict/gpa_mean_shape_plant_predict.png\n",
      "\n",
      "--- Performing Full PCA and Generating Explained Variance Report ---\n",
      "PC1: 61.07%, 61.07%\n",
      "PC2: 17.0%, 78.06%\n",
      "PC3: 9.46%, 87.52%\n",
      "PC4: 3.03%, 90.55%\n",
      "PC5: 1.79%, 92.34%\n",
      "PC6: 1.19%, 93.53%\n",
      "PC7: 1.05%, 94.58%\n",
      "PC8: 0.68%, 95.26%\n",
      "PC9: 0.62%, 95.88%\n",
      "PC10: 0.51%, 96.39%\n",
      "PC11: 0.41%, 96.8%\n",
      "PC12: 0.31%, 97.11%\n",
      "PC13: 0.3%, 97.41%\n",
      "PC14: 0.26%, 97.67%\n",
      "PC15: 0.21%, 97.88%\n",
      "PC16: 0.19%, 98.07%\n",
      "PC17: 0.16%, 98.23%\n",
      "PC18: 0.13%, 98.35%\n",
      "PC19: 0.12%, 98.47%\n",
      "PC20: 0.11%, 98.57%\n",
      "PC21: 0.09%, 98.66%\n",
      "PC22: 0.07%, 98.74%\n",
      "PC23: 0.07%, 98.81%\n",
      "PC24: 0.07%, 98.88%\n",
      "PC25: 0.06%, 98.93%\n",
      "PC26: 0.05%, 98.98%\n",
      "PC27: 0.05%, 99.03%\n",
      "PC28: 0.05%, 99.08%\n",
      "PC29: 0.04%, 99.12%\n",
      "PC30: 0.04%, 99.16%\n",
      "PC31: 0.03%, 99.19%\n",
      "PC32: 0.03%, 99.23%\n",
      "PC33: 0.03%, 99.25%\n",
      "PC34: 0.03%, 99.28%\n",
      "PC35: 0.03%, 99.31%\n",
      "PC36: 0.02%, 99.33%\n",
      "PC37: 0.02%, 99.35%\n",
      "PC38: 0.02%, 99.38%\n",
      "PC39: 0.02%, 99.4%\n",
      "PC40: 0.02%, 99.42%\n",
      "PC41: 0.02%, 99.43%\n",
      "PC42: 0.02%, 99.45%\n",
      "PC43: 0.02%, 99.47%\n",
      "PC44: 0.02%, 99.48%\n",
      "PC45: 0.01%, 99.49%\n",
      "PC46: 0.01%, 99.51%\n",
      "PC47: 0.01%, 99.52%\n",
      "PC48: 0.01%, 99.53%\n",
      "PC49: 0.01%, 99.54%\n",
      "PC50: 0.01%, 99.56%\n",
      "PC51: 0.01%, 99.57%\n",
      "PC52: 0.01%, 99.58%\n",
      "PC53: 0.01%, 99.59%\n",
      "PC54: 0.01%, 99.6%\n",
      "PC55: 0.01%, 99.61%\n",
      "PC56: 0.01%, 99.62%\n",
      "PC57: 0.01%, 99.62%\n",
      "PC58: 0.01%, 99.63%\n",
      "PC59: 0.01%, 99.64%\n",
      "PC60: 0.01%, 99.65%\n",
      "PC61: 0.01%, 99.66%\n",
      "PC62: 0.01%, 99.67%\n",
      "PC63: 0.01%, 99.67%\n",
      "PC64: 0.01%, 99.68%\n",
      "PC65: 0.01%, 99.69%\n",
      "PC66: 0.01%, 99.69%\n",
      "PC67: 0.01%, 99.7%\n",
      "PC68: 0.01%, 99.71%\n",
      "PC69: 0.01%, 99.72%\n",
      "PC70: 0.01%, 99.72%\n",
      "PC71: 0.01%, 99.73%\n",
      "PC72: 0.01%, 99.73%\n",
      "PC73: 0.01%, 99.74%\n",
      "PC74: 0.01%, 99.75%\n",
      "PC75: 0.01%, 99.75%\n",
      "PC76: 0.01%, 99.76%\n",
      "PC77: 0.01%, 99.76%\n",
      "PC78: 0.01%, 99.77%\n",
      "PC79: 0.01%, 99.78%\n",
      "PC80: 0.01%, 99.78%\n",
      "PC81: 0.01%, 99.79%\n",
      "PC82: 0.01%, 99.79%\n",
      "PC83: 0.01%, 99.8%\n",
      "PC84: 0.01%, 99.8%\n",
      "PC85: 0.01%, 99.81%\n",
      "PC86: 0.01%, 99.81%\n",
      "PC87: 0.0%, 99.82%\n",
      "PC88: 0.0%, 99.82%\n",
      "PC89: 0.0%, 99.83%\n",
      "PC90: 0.0%, 99.83%\n",
      "PC91: 0.0%, 99.84%\n",
      "PC92: 0.0%, 99.84%\n",
      "PC93: 0.0%, 99.85%\n",
      "PC94: 0.0%, 99.85%\n",
      "PC95: 0.0%, 99.86%\n",
      "PC96: 0.0%, 99.86%\n",
      "PC97: 0.0%, 99.86%\n",
      "PC98: 0.0%, 99.87%\n",
      "PC99: 0.0%, 99.87%\n",
      "PC100: 0.0%, 99.88%\n",
      "PC101: 0.0%, 99.88%\n",
      "PC102: 0.0%, 99.88%\n",
      "PC103: 0.0%, 99.89%\n",
      "PC104: 0.0%, 99.89%\n",
      "PC105: 0.0%, 99.9%\n",
      "PC106: 0.0%, 99.9%\n",
      "PC107: 0.0%, 99.9%\n",
      "PC108: 0.0%, 99.91%\n",
      "PC109: 0.0%, 99.91%\n",
      "PC110: 0.0%, 99.91%\n",
      "PC111: 0.0%, 99.92%\n",
      "PC112: 0.0%, 99.92%\n",
      "PC113: 0.0%, 99.92%\n",
      "PC114: 0.0%, 99.93%\n",
      "PC115: 0.0%, 99.93%\n",
      "PC116: 0.0%, 99.93%\n",
      "PC117: 0.0%, 99.94%\n",
      "PC118: 0.0%, 99.94%\n",
      "PC119: 0.0%, 99.94%\n",
      "PC120: 0.0%, 99.95%\n",
      "PC121: 0.0%, 99.95%\n",
      "PC122: 0.0%, 99.95%\n",
      "PC123: 0.0%, 99.96%\n",
      "PC124: 0.0%, 99.96%\n",
      "PC125: 0.0%, 99.96%\n",
      "PC126: 0.0%, 99.96%\n",
      "PC127: 0.0%, 99.96%\n",
      "PC128: 0.0%, 99.97%\n",
      "PC129: 0.0%, 99.97%\n",
      "PC130: 0.0%, 99.97%\n",
      "PC131: 0.0%, 99.97%\n",
      "PC132: 0.0%, 99.97%\n",
      "PC133: 0.0%, 99.97%\n",
      "PC134: 0.0%, 99.98%\n",
      "PC135: 0.0%, 99.98%\n",
      "PC136: 0.0%, 99.98%\n",
      "PC137: 0.0%, 99.98%\n",
      "PC138: 0.0%, 99.98%\n",
      "PC139: 0.0%, 99.98%\n",
      "PC140: 0.0%, 99.98%\n",
      "PC141: 0.0%, 99.98%\n",
      "PC142: 0.0%, 99.98%\n",
      "PC143: 0.0%, 99.98%\n",
      "PC144: 0.0%, 99.99%\n",
      "PC145: 0.0%, 99.99%\n",
      "PC146: 0.0%, 99.99%\n",
      "PC147: 0.0%, 99.99%\n",
      "PC148: 0.0%, 99.99%\n",
      "PC149: 0.0%, 99.99%\n",
      "PC150: 0.0%, 99.99%\n",
      "PC151: 0.0%, 99.99%\n",
      "PC152: 0.0%, 99.99%\n",
      "PC153: 0.0%, 99.99%\n",
      "PC154: 0.0%, 99.99%\n",
      "PC155: 0.0%, 99.99%\n",
      "PC156: 0.0%, 99.99%\n",
      "PC157: 0.0%, 99.99%\n",
      "PC158: 0.0%, 99.99%\n",
      "PC159: 0.0%, 99.99%\n",
      "PC160: 0.0%, 99.99%\n",
      "PC161: 0.0%, 99.99%\n",
      "PC162: 0.0%, 99.99%\n",
      "PC163: 0.0%, 99.99%\n",
      "PC164: 0.0%, 100.0%\n",
      "PC165: 0.0%, 100.0%\n",
      "PC166: 0.0%, 100.0%\n",
      "PC167: 0.0%, 100.0%\n",
      "PC168: 0.0%, 100.0%\n",
      "PC169: 0.0%, 100.0%\n",
      "PC170: 0.0%, 100.0%\n",
      "PC171: 0.0%, 100.0%\n",
      "PC172: 0.0%, 100.0%\n",
      "PC173: 0.0%, 100.0%\n",
      "PC174: 0.0%, 100.0%\n",
      "PC175: 0.0%, 100.0%\n",
      "PC176: 0.0%, 100.0%\n",
      "PC177: 0.0%, 100.0%\n",
      "PC178: 0.0%, 100.0%\n",
      "PC179: 0.0%, 100.0%\n",
      "PC180: 0.0%, 100.0%\n",
      "PC181: 0.0%, 100.0%\n",
      "PC182: 0.0%, 100.0%\n",
      "PC183: 0.0%, 100.0%\n",
      "PC184: 0.0%, 100.0%\n",
      "PC185: 0.0%, 100.0%\n",
      "PC186: 0.0%, 100.0%\n",
      "PC187: 0.0%, 100.0%\n",
      "PC188: 0.0%, 100.0%\n",
      "PC189: 0.0%, 100.0%\n",
      "PC190: 0.0%, 100.0%\n",
      "PC191: 0.0%, 100.0%\n",
      "PC192: 0.0%, 100.0%\n",
      "PC193: 0.0%, 100.0%\n",
      "PC194: 0.0%, 100.0%\n",
      "PC195: 0.0%, 100.0%\n",
      "PC196: 0.0%, 100.0%\n",
      "PC197: 0.0%, 100.0%\n",
      "PC198: 0.0%, 100.0%\n",
      "PCA explained variance report saved to ./03_morphometrics_output_plant_predict/pca_explained_variance_plant_predict.txt\n",
      "\n",
      "--- Saving PCA model parameters, PC scores, and class labels ---\n",
      "  PCA Components shape: (198, 198)\n",
      "  PCA Mean shape: (198,)\n",
      "  PCA Explained Variance shape: (198,)\n",
      "  PCA Explained Variance Ratio shape: (198,)\n",
      "  Number of PCA components: 198\n",
      "  Original PCA Scores (PCs) shape: (3246, 198)\n",
      "  Class Labels (plantID) length: 3246\n",
      "PCA parameters saved to ./03_morphometrics_output_plant_predict/leaf_pca_model_parameters_plant_predict.h5\n",
      "Original PCA scores, class labels, AND original flattened coordinates saved to ./03_morphometrics_output_plant_predict/original_pca_scores_and_class_labels_plant_predict.h5\n",
      "\n",
      "========== Analysis for PLANT_PREDICT Dataset Completed ==========\n",
      "\n",
      "All analyses for the plant prediction dataset completed and outputs saved.\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "### LOAD IN MODULES ###\n",
    "#######################\n",
    "\n",
    "import cv2\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import procrustes\n",
    "from scipy.spatial import ConvexHull # Not directly used in the provided segment, but kept for completeness\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # Not directly used, but kept\n",
    "from sklearn.metrics import confusion_matrix # Not directly used, but kept\n",
    "import scipy.stats as stats # Not directly used, but kept\n",
    "import statsmodels.stats.multitest as multitest # Not directly used, but kept\n",
    "import itertools # Not directly used, but kept\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns # Used, but for plotting commented out\n",
    "from matplotlib.colors import LogNorm # Not directly used, but kept\n",
    "import phate # Not directly used, but kept\n",
    "import scprep # Not directly used, but kept\n",
    "import h5py\n",
    "import pickle # Not directly used, but kept\n",
    "import os\n",
    "\n",
    "#################\n",
    "### FUNCTIONS ###\n",
    "#################\n",
    "\n",
    "def angle_between(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    define a function to find the angle between 3 points anti-clockwise in degrees, p2 being the vertex\n",
    "    inputs: three angle points, as tuples\n",
    "    output: angle in degrees\n",
    "    \"\"\"\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    x3, y3 = p3\n",
    "    deg1 = (360 + math.degrees(math.atan2(x1 - x2, y1 - y2))) % 360\n",
    "    deg2 = (360 + math.degrees(math.atan2(x3 - x2, y3 - y2))) % 360\n",
    "    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)\n",
    "\n",
    "def rotate_points(xvals, yvals, degrees):\n",
    "    \"\"\"\"\n",
    "    define a function to rotate 2D x and y coordinate points around the origin\n",
    "    inputs: x and y vals (can take pandas dataframe columns) and the degrees (positive, anticlockwise) to rotate\n",
    "    outputs: rotated and y vals\n",
    "    \"\"\"\n",
    "    angle_to_move = 90 - degrees\n",
    "    rads = np.deg2rad(angle_to_move)\n",
    "\n",
    "    new_xvals = xvals * np.cos(rads) - yvals * np.sin(rads)\n",
    "    new_yvals = xvals * np.sin(rads) + yvals * np.cos(rads)\n",
    "\n",
    "    return new_xvals, new_yvals\n",
    "\n",
    "def interpolation(x, y, number):\n",
    "    \"\"\"\n",
    "    define a function to return equally spaced, interpolated points for a given polyline\n",
    "    inputs: arrays of x and y values for a polyline, number of points to interpolate\n",
    "    ouputs: interpolated points along the polyline, inclusive of start and end points\n",
    "    \"\"\"\n",
    "    if len(x) < 2 or len(y) < 2:\n",
    "        if np.all(x == x[0]) and np.all(y == y[0]):\n",
    "            return np.full(number, x[0]), np.full(number, y[0])\n",
    "        elif len(x) == 1:\n",
    "            return np.full(number, x[0]), np.full(number, y[0])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    distance = np.cumsum(np.sqrt(np.ediff1d(x, to_begin=0)**2 + np.ediff1d(y, to_begin=0)**2))\n",
    "\n",
    "    if distance[-1] == 0:\n",
    "        return np.full(number, x[0]), np.full(number, y[0])\n",
    "\n",
    "    distance = distance / distance[-1]\n",
    "\n",
    "    fx, fy = interp1d(distance, x), interp1d(distance, y)\n",
    "\n",
    "    alpha = np.linspace(0, 1, number)\n",
    "    x_regular, y_regular = fx(alpha), fy(alpha)\n",
    "\n",
    "    return x_regular, y_regular\n",
    "\n",
    "def euclid_dist(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    define a function to return the euclidean distance between two points\n",
    "    inputs: x and y values of the two points\n",
    "    output: the eulidean distance\n",
    "    \"\"\"\n",
    "    return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def poly_area(x, y):\n",
    "    \"\"\"\n",
    "    define a function to calculate the area of a polygon using the shoelace algorithm\n",
    "    inputs: separate numpy arrays of x and y coordinate values\n",
    "    outputs: the area of the polygon\n",
    "    \"\"\"\n",
    "    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n",
    "\n",
    "def gpa_mean(leaf_arr, landmark_num, dim_num):\n",
    "    \"\"\"\n",
    "    define a function that given an array of landmark data returns the Generalized Procrustes Analysis mean\n",
    "    inputs: a 3 dimensional array of samples by landmarks by coordinate values, number of landmarks, number of dimensions\n",
    "    output: an array of the Generalized Procrustes Analysis mean shape\n",
    "    \"\"\"\n",
    "    ref_ind = 0\n",
    "    ref_shape = leaf_arr[ref_ind, :, :]\n",
    "    mean_diff = 10**(-30)\n",
    "    old_mean = ref_shape\n",
    "    d = 1000000\n",
    "\n",
    "    while d > mean_diff:\n",
    "        arr = np.zeros(((len(leaf_arr)), landmark_num, dim_num))\n",
    "        for i in range(len(leaf_arr)):\n",
    "            s1, s2, distance = procrustes(old_mean, leaf_arr[i])\n",
    "            arr[i] = s2\n",
    "        new_mean = np.mean(arr, axis=(0))\n",
    "        s1, s2, d = procrustes(old_mean, new_mean)\n",
    "        old_mean = new_mean\n",
    "    return new_mean\n",
    "\n",
    "def run_morphometric_analysis(metadata_file_path, image_data_dir, output_base_dir, dataset_name):\n",
    "    \"\"\"\n",
    "    Runs the full morphometric analysis pipeline for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        metadata_file_path (str): Path to the CSV metadata file.\n",
    "        image_data_dir (str): Path to the directory containing image files.\n",
    "        output_base_dir (str): Base directory where all outputs for this dataset will be saved.\n",
    "        dataset_name (str): A descriptive name for the dataset (e.g., \"plant_predict\")\n",
    "                            used in print statements and specific output filenames.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*10} Starting Analysis for {dataset_name.upper()} Dataset {'='*10}\")\n",
    "\n",
    "    # --- Configuration and Inputs (now passed as arguments or derived) ---\n",
    "    # Parameters for Preprocessing\n",
    "    HIGH_RES_INTERPOLATION_POINTS = 10000\n",
    "    FINAL_PSEUDO_LANDMARKS_PER_SIDE = 50\n",
    "    NUM_LANDMARKS = (FINAL_PSEUDO_LANDMARKS_PER_SIDE * 2) - 1\n",
    "    NUM_DIMENSIONS = 2\n",
    "\n",
    "    # Parameters for Morphospace Visualization (2-Component PCA) - mostly commented out, but parameters remain if needed\n",
    "    # MORPHOSPACE_PLOT_LENGTH = 10\n",
    "    # MORPHOSPACE_PLOT_WIDTH = 10\n",
    "    # MORPHOSPACE_PC1_INTERVALS = 20\n",
    "    # MORPHOSPACE_PC2_INTERVALS = 6\n",
    "    MORPHOSPACE_HUE_COLUMN = \"plantID\" # !!! CHANGED to plantID !!!\n",
    "    # EIGENLEAF_SCALE = 0.08\n",
    "    # EIGENLEAF_COLOR = \"lightgray\"\n",
    "    # EIGENLEAF_ALPHA = 0.5\n",
    "    # POINT_SIZE = 80\n",
    "    # POINT_LINEWIDTH = 0\n",
    "    # POINT_ALPHA = 0.6\n",
    "    # AXIS_LABEL_FONTSIZE = 12\n",
    "    # AXIS_TICK_FONTSIZE = 8\n",
    "    # FACE_COLOR = \"white\"\n",
    "    # GRID_ALPHA = 0.5\n",
    "\n",
    "    # Parameters for Output Files\n",
    "    GPA_MEAN_SHAPE_PLOT_FILENAME = f\"gpa_mean_shape_{dataset_name}.png\"\n",
    "    PCA_EXPLAINED_VARIANCE_REPORT_FILENAME = f\"pca_explained_variance_{dataset_name}.txt\"\n",
    "    MORPHOSPACE_PLOT_FILENAME = f\"morphospace_plot_{dataset_name}.png\" # Retained filename, but plot is not generated\n",
    "    PCA_PARAMS_H5_FILENAME = f\"leaf_pca_model_parameters_{dataset_name}.h5\"\n",
    "    ORIGINAL_PCA_SCORES_AND_LABELS_H5_FILENAME = f\"original_pca_scores_and_class_labels_{dataset_name}.h5\"\n",
    "    CLASS_LABEL_COLUMN_FOR_SAVING = \"plantID\" # !!! CHANGED to plantID !!!\n",
    "\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    print(f\"Saving outputs to directory: {output_base_dir}\")\n",
    "\n",
    "    # --- Read in Metadata ---\n",
    "    mdata = pd.read_csv(metadata_file_path)\n",
    "    print(f\"Metadata loaded from: {metadata_file_path}\")\n",
    "    print(\"First 5 rows of loaded metadata:\")\n",
    "    print(mdata.head())\n",
    "\n",
    "    # --- Print plantID counts and structure ---\n",
    "    print(\"\\n--- Plant ID Class Information ---\")\n",
    "    if 'plantID' in mdata.columns and 'full_name' in mdata.columns:\n",
    "        print(\"\\nNumber of different plantID classes for each full_name (variety):\")\n",
    "        plant_id_counts_per_variety = mdata.groupby('full_name')['plantID'].nunique()\n",
    "        print(plant_id_counts_per_variety.sort_index().to_string())\n",
    "\n",
    "        print(f\"\\nTotal number of unique plantID classes: {mdata['plantID'].nunique()}\")\n",
    "        print(\"\\nCounts of each plantID class:\")\n",
    "        plant_id_counts_overall = mdata['plantID'].value_counts().sort_index()\n",
    "        print(plant_id_counts_overall.to_string())\n",
    "    else:\n",
    "        print(\"Warning: 'plantID' or 'full_name' column not found in metadata. Cannot print class information.\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "    # --- Make a list of image file names ---\n",
    "    file_names = mdata['file'].tolist()\n",
    "    file_names.sort()\n",
    "    print(f\"Found {len(file_names)} image files to process from metadata.\")\n",
    "\n",
    "    # --- Interpolate Points Creating Pseudo-Landmarks and Pre-process ---\n",
    "    print(\"\\n--- Preprocessing Images and Interpolating Pseudo-Landmarks ---\")\n",
    "    \n",
    "    # Filter out rows with missing image files *before* the loop to avoid incomplete arrays\n",
    "    existing_image_files = set(f for f in os.listdir(image_data_dir) if os.path.isfile(os.path.join(image_data_dir, f)))\n",
    "    \n",
    "    valid_rows_indices = []\n",
    "    processed_points_list = []    \n",
    "\n",
    "    for lf_idx, row in mdata.iterrows():\n",
    "        curr_image_filename = row[\"file\"]\n",
    "        img_path = os.path.join(image_data_dir, curr_image_filename)\n",
    "\n",
    "        if curr_image_filename not in existing_image_files:\n",
    "            print(f\"Warning: Image file not found at {img_path}. Skipping and will exclude from analysis.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img = cv2.bitwise_not(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2GRAY))\n",
    "            contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            x_conts = []\n",
    "            y_conts = []\n",
    "            areas_conts = []\n",
    "            for c in contours:\n",
    "                x_vals = [pt[0][0] for pt in c]\n",
    "                y_vals = [pt[0][1] for pt in c]\n",
    "                area = (max(x_vals) - min(x_vals)) * (max(y_vals) - min(y_vals))\n",
    "                x_conts.append(x_vals)\n",
    "                y_conts.append(y_vals)\n",
    "                areas_conts.append(area)\n",
    "\n",
    "            if not areas_conts:\n",
    "                print(f\"Warning: No contours found for image {curr_image_filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            area_inds = np.flip(np.argsort(areas_conts))\n",
    "            sorted_x_conts = np.array(x_conts, dtype=object)[area_inds][0:]\n",
    "            sorted_y_conts = np.array(y_conts, dtype=object)[area_inds][0:]\n",
    "\n",
    "            high_res_x, high_res_y = interpolation(np.array(sorted_x_conts[0], dtype=np.float32),\n",
    "                                                   np.array(sorted_y_conts[0], dtype=np.float32), HIGH_RES_INTERPOLATION_POINTS)\n",
    "\n",
    "            base_pt = np.array((row[\"base_x\"], row[\"base_y\"]))\n",
    "            tip_pt = np.array((row[\"tip_x\"], row[\"tip_y\"]))\n",
    "\n",
    "            base_dists = [euclid_dist(base_pt[0], base_pt[1], high_res_x[pt], high_res_y[pt]) for pt in range(len(high_res_x))]\n",
    "            tip_dists = [euclid_dist(tip_pt[0], tip_pt[1], high_res_x[pt], high_res_y[pt]) for pt in range(len(high_res_x))]\n",
    "\n",
    "            base_ind = np.argmin(base_dists)\n",
    "            tip_ind = np.argmin(tip_dists)\n",
    "\n",
    "            high_res_x = np.concatenate((high_res_x[base_ind:], high_res_x[:base_ind]))\n",
    "            high_res_y = np.concatenate((high_res_y[base_ind:], high_res_y[:base_ind]))\n",
    "\n",
    "            new_tip_dists = [euclid_dist(tip_pt[0], tip_pt[1], high_res_x[pt_idx], high_res_y[pt_idx]) for pt_idx in range(len(high_res_x))]\n",
    "            tip_ind_new = np.argmin(new_tip_dists)\n",
    "\n",
    "            lf_contour = np.column_stack((high_res_x, high_res_y))\n",
    "\n",
    "            left_segment = lf_contour[0:tip_ind_new + 1, :]\n",
    "            right_segment = np.concatenate((lf_contour[tip_ind_new:, :], lf_contour[0:1, :]), axis=0)\n",
    "\n",
    "            if len(left_segment) < 2 or len(right_segment) < 2:\n",
    "                print(f\"Warning: Segments for image {curr_image_filename} are too short for interpolation. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            left_inter_x, left_inter_y = interpolation(left_segment[:, 0], left_segment[:, 1], FINAL_PSEUDO_LANDMARKS_PER_SIDE)\n",
    "            right_inter_x, right_inter_y = interpolation(right_segment[:, 0], right_segment[:, 1], FINAL_PSEUDO_LANDMARKS_PER_SIDE)\n",
    "\n",
    "            left_inter_x = np.delete(left_inter_x, -1)\n",
    "            left_inter_y = np.delete(left_inter_y, -1)\n",
    "\n",
    "            lf_pts_left = np.column_stack((left_inter_x, left_inter_y))\n",
    "            lf_pts_right = np.column_stack((right_inter_x, right_inter_y))\n",
    "            lf_pts = np.row_stack((lf_pts_left, lf_pts_right))\n",
    "\n",
    "            if lf_pts.shape[0] != NUM_LANDMARKS:\n",
    "                print(f\"Warning: Leaf {curr_image_filename} generated {lf_pts.shape[0]} landmarks, expected {NUM_LANDMARKS}. Check interpolation logic.\")\n",
    "                continue\n",
    "\n",
    "            tip_point = lf_pts[FINAL_PSEUDO_LANDMARKS_PER_SIDE - 1, :]\n",
    "            base_point = lf_pts[0, :]\n",
    "\n",
    "            ang = angle_between(tip_point, base_point, (base_point[0] + 1, base_point[1]))\n",
    "\n",
    "            rot_x, rot_y = rotate_points(lf_pts[:, 0], lf_pts[:, 1], ang)\n",
    "            rot_pts = np.column_stack((rot_x, rot_y))\n",
    "\n",
    "            processed_points_list.append(rot_pts)\n",
    "            valid_rows_indices.append(lf_idx)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {curr_image_filename}: {e}. Skipping.\")\n",
    "            continue\n",
    "    \n",
    "    # Rebuild mdata and cult_cm_arr with only successfully processed images\n",
    "    mdata = mdata.iloc[valid_rows_indices].reset_index(drop=True)\n",
    "    cult_cm_arr = np.array(processed_points_list)\n",
    "\n",
    "    if cult_cm_arr.shape[0] == 0:\n",
    "        print(f\"No valid images processed for {dataset_name}. Exiting analysis for this dataset.\")\n",
    "        return\n",
    "\n",
    "    # --- Calculate GPA Mean ---\n",
    "    print(\"--- Calculating GPA Mean ---\")\n",
    "    mean_shape = gpa_mean(cult_cm_arr, NUM_LANDMARKS, NUM_DIMENSIONS)\n",
    "\n",
    "    # --- Align Leaves to GPA Mean ---\n",
    "    print(\"--- Aligning Leaves to GPA Mean ---\")\n",
    "    proc_arr = np.zeros(np.shape(cult_cm_arr))\n",
    "    for i in range(len(cult_cm_arr)):\n",
    "        s1, s2, distance = procrustes(mean_shape, cult_cm_arr[i, :, :])\n",
    "        s2_normalized = s2 # Procrustes already scales and translates, but often further normalization to unit centroid size is done if not handled by procrustes. For now, keep as is.\n",
    "        proc_arr[i] = s2_normalized\n",
    "\n",
    "    # --- Visualize GPA Aligned Shapes and Mean ---\n",
    "    print(\"--- Visualizing GPA Aligned Shapes ---\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(len(proc_arr)):\n",
    "        plt.plot(proc_arr[i, :, 0], proc_arr[i, :, 1], c=\"k\", alpha=0.08)\n",
    "    plt.plot(np.mean(proc_arr, axis=0)[:, 0], np.mean(proc_arr, axis=0)[:, 1], c=\"magenta\")\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Procrustes Aligned Leaf Shapes and GPA Mean ({dataset_name.replace('_', ' ').title()})\")\n",
    "    plt.savefig(os.path.join(output_base_dir, GPA_MEAN_SHAPE_PLOT_FILENAME))\n",
    "    plt.close()\n",
    "    print(f\"GPA mean shape plot saved to {os.path.join(output_base_dir, GPA_MEAN_SHAPE_PLOT_FILENAME)}\")\n",
    "\n",
    "    # --- Calculate Percent Variance All PCs ---\n",
    "    print(\"\\n--- Performing Full PCA and Generating Explained Variance Report ---\")\n",
    "    flat_arr = proc_arr.reshape(np.shape(proc_arr)[0], np.shape(proc_arr)[1] * np.shape(proc_arr)[2])\n",
    "\n",
    "    max_pc_components = min(flat_arr.shape[0], flat_arr.shape[1])\n",
    "    pca = PCA(n_components=max_pc_components)\n",
    "    PCs = pca.fit_transform(flat_arr)\n",
    "\n",
    "    pca_explained_variance_filepath = os.path.join(output_base_dir, PCA_EXPLAINED_VARIANCE_REPORT_FILENAME)\n",
    "    with open(pca_explained_variance_filepath, 'w') as f:\n",
    "        f.write(f\"PCA Explained Variance Report ({dataset_name.replace('_', ' ').title()} Dataset):\\n\")\n",
    "        f.write(f\"Total Samples: {flat_arr.shape[0]}\\n\")\n",
    "        f.write(f\"Total Features (landmarks * dimensions): {flat_arr.shape[1]}\\n\")\n",
    "        f.write(f\"Number of PCs Calculated: {pca.n_components_}\\n\\n\")\n",
    "        f.write(\"PC: var, overall\\n\")\n",
    "        for i in range(len(pca.explained_variance_ratio_)):\n",
    "            pc_variance = round(pca.explained_variance_ratio_[i] * 100, 2)\n",
    "            cumulative_variance = round(pca.explained_variance_ratio_.cumsum()[i] * 100, 2)\n",
    "            line = f\"PC{i+1}: {pc_variance}%, {cumulative_variance}%\\n\"\n",
    "            print(line.strip())\n",
    "            f.write(line)\n",
    "    print(f\"PCA explained variance report saved to {pca_explained_variance_filepath}\")\n",
    "\n",
    "    # --- Save PCA Model Parameters, PC Scores, and Class Labels ---\n",
    "    print(\"\\n--- Saving PCA model parameters, PC scores, and class labels ---\")\n",
    "    pca_components = pca.components_\n",
    "    pca_mean = pca.mean_\n",
    "    pca_explained_variance = pca.explained_variance_\n",
    "    pca_explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    n_pca_components = pca.n_components_\n",
    "\n",
    "    print(f\"  PCA Components shape: {pca_components.shape}\")\n",
    "    print(f\"  PCA Mean shape: {pca_mean.shape}\")\n",
    "    print(f\"  PCA Explained Variance shape: {pca_explained_variance.shape}\")\n",
    "    print(f\"  PCA Explained Variance Ratio shape: {pca_explained_variance_ratio.shape}\")\n",
    "    print(f\"  Number of PCA components: {n_pca_components}\")\n",
    "    print(f\"  Original PCA Scores (PCs) shape: {PCs.shape}\")\n",
    "    print(f\"  Class Labels ({CLASS_LABEL_COLUMN_FOR_SAVING}) length: {len(mdata[CLASS_LABEL_COLUMN_FOR_SAVING])}\")\n",
    "\n",
    "    pca_params_filepath = os.path.join(output_base_dir, PCA_PARAMS_H5_FILENAME)\n",
    "    with h5py.File(pca_params_filepath, 'w') as f:\n",
    "        f.create_dataset('components', data=pca_components, compression=\"gzip\")\n",
    "        f.create_dataset('mean', data=pca_mean, compression=\"gzip\")\n",
    "        f.create_dataset('explained_variance', data=pca_explained_variance, compression=\"gzip\")\n",
    "        f.create_dataset('explained_variance_ratio', data=pca_explained_variance_ratio, compression=\"gzip\")\n",
    "        f.attrs['n_components'] = n_pca_components\n",
    "    print(f\"PCA parameters saved to {pca_params_filepath}\")\n",
    "\n",
    "    pca_scores_labels_filepath = os.path.join(output_base_dir, ORIGINAL_PCA_SCORES_AND_LABELS_H5_FILENAME)\n",
    "    with h5py.File(pca_scores_labels_filepath, 'w') as f:\n",
    "        f.create_dataset('pca_scores', data=PCs, compression=\"gzip\")\n",
    "        # Ensure class labels are stored as variable-length strings ('S')\n",
    "        f.create_dataset('class_labels', data=np.array(mdata[CLASS_LABEL_COLUMN_FOR_SAVING]).astype('S'), compression=\"gzip\")\n",
    "        f.create_dataset('original_flattened_coords', data=flat_arr, compression=\"gzip\")\n",
    "    print(f\"Original PCA scores, class labels, AND original flattened coordinates saved to {pca_scores_labels_filepath}\")\n",
    "\n",
    "    # --- Create Morphospace (COMMENTED OUT as requested) ---\n",
    "    # print(\"\\n--- Creating Morphospace Plot ---\")\n",
    "    # morphospace_pca = PCA(n_components=2)\n",
    "    # morphospace_PCs = morphospace_pca.fit_transform(flat_arr)\n",
    "\n",
    "    # mdata[\"PC1\"] = morphospace_PCs[:, 0]\n",
    "    # mdata[\"PC2\"] = morphospace_PCs[:, 1]\n",
    "\n",
    "    # plt.figure(figsize=(MORPHOSPACE_PLOT_LENGTH, MORPHOSPACE_PLOT_WIDTH))\n",
    "    # plt.gca().set_facecolor(FACE_COLOR)\n",
    "    # plt.gca().set_axisbelow(True)\n",
    "\n",
    "    # PC1_vals = np.linspace(np.min(PCs[:, 0]), np.max(PCs[:, 0]), MORPHOSPACE_PC1_INTERVALS)\n",
    "    # PC2_vals = np.linspace(np.min(PCs[:, 1]), np.max(PCs[:, 1]), MORPHOSPACE_PC2_INTERVALS)\n",
    "\n",
    "    # for i in PC1_vals:\n",
    "    #     for j in PC2_vals:\n",
    "    #         inv_leaf = morphospace_pca.inverse_transform(np.array([i, j]))\n",
    "    #         inv_leaf_coords = inv_leaf.reshape(NUM_LANDMARKS, NUM_DIMENSIONS)\n",
    "\n",
    "    #         inv_x = inv_leaf_coords[:, 0]\n",
    "    #         inv_y = inv_leaf_coords[:, 1]\n",
    "\n",
    "    #         plt.fill(inv_x * EIGENLEAF_SCALE + i, inv_y * EIGENLEAF_SCALE + j,\n",
    "    #                  c=EIGENLEAF_COLOR, alpha=EIGENLEAF_ALPHA)\n",
    "\n",
    "    # sns.scatterplot(data=mdata, x=\"PC1\", y=\"PC2\", hue=MORPHOSPACE_HUE_COLUMN,\n",
    "    #                 s=POINT_SIZE, linewidth=POINT_LINEWIDTH, alpha=POINT_ALPHA)\n",
    "\n",
    "    # plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9})\n",
    "    # xlab = f\"PC1, {round(pca.explained_variance_ratio_[0] * 100, 1)}%\"\n",
    "    # ylab = f\"PC2, {round(pca.explained_variance_ratio_[1] * 100, 1)}%\"\n",
    "    # plt.xlabel(xlab, fontsize=AXIS_LABEL_FONTSIZE)\n",
    "    # plt.ylabel(ylab, fontsize=AXIS_LABEL_FONTSIZE)\n",
    "    # plt.xticks(fontsize=AXIS_TICK_FONTSIZE)\n",
    "    # plt.yticks(fontsize=AXIS_TICK_FONTSIZE)\n",
    "    # plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "    # plt.savefig(os.path.join(output_base_dir, MORPHOSPACE_PLOT_FILENAME), bbox_inches='tight')\n",
    "    # plt.close()\n",
    "    # print(f\"Morphospace plot saved to {os.path.join(output_base_dir, MORPHOSPACE_PLOT_FILENAME)}\")\n",
    "\n",
    "    print(f\"\\n{'='*10} Analysis for {dataset_name.upper()} Dataset Completed {'='*10}\")\n",
    "\n",
    "\n",
    "# --- Define paths relative to the current notebook (COCA_PROJECT/data/PLANTPREDICT/) ---\n",
    "# Paths for the plant prediction dataset\n",
    "PLANT_PREDICT_METADATA_FILE = \"./01_cultivated2nd_landmarks.csv\" # In the same directory as the script\n",
    "PLANT_PREDICT_IMAGE_DIR = \"../../data/CULTIVATED2ND/00_cultivated2nd_data/\" # Up two levels to COCA_PROJECT, then into data/CULTIVATED2ND/\n",
    "PLANT_PREDICT_OUTPUT_DIR = \"./03_morphometrics_output_plant_predict/\" # New unique output dir within PLANTPREDICT\n",
    "\n",
    "# --- Run analysis for the plant prediction dataset ---\n",
    "run_morphometric_analysis(\n",
    "    metadata_file_path=PLANT_PREDICT_METADATA_FILE,\n",
    "    image_data_dir=PLANT_PREDICT_IMAGE_DIR,\n",
    "    output_base_dir=PLANT_PREDICT_OUTPUT_DIR,\n",
    "    dataset_name=\"plant_predict\" # Descriptive name for this analysis\n",
    ")\n",
    "\n",
    "print(\"\\nAll analyses for the plant prediction dataset completed and outputs saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459628f4-e1a6-4198-8d85-ef2660023d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
