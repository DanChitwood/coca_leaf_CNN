{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043b1619-a73f-47de-9e20-0c965f19692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving outputs to directory: ./03_morphometrics_output/\n",
      "\n",
      "--- Preprocessing Images and Interpolating Pseudo-Landmarks ---\n",
      "\n",
      "--- Calculating GPA Mean ---\n",
      "--- Aligning Leaves to GPA Mean ---\n",
      "--- Visualizing GPA Aligned Shapes ---\n",
      "GPA mean shape plot saved to ./03_morphometrics_output/gpa_mean_shape.png\n",
      "\n",
      "--- Performing Full PCA and Generating Explained Variance Report ---\n",
      "PC1: 77.39%, 77.39%\n",
      "PC2: 11.04%, 88.43%\n",
      "PC3: 3.56%, 92.0%\n",
      "PC4: 2.06%, 94.06%\n",
      "PC5: 1.63%, 95.69%\n",
      "PC6: 0.92%, 96.61%\n",
      "PC7: 0.69%, 97.3%\n",
      "PC8: 0.47%, 97.77%\n",
      "PC9: 0.43%, 98.2%\n",
      "PC10: 0.31%, 98.51%\n",
      "PC11: 0.25%, 98.76%\n",
      "PC12: 0.22%, 98.98%\n",
      "PC13: 0.16%, 99.14%\n",
      "PC14: 0.12%, 99.27%\n",
      "PC15: 0.09%, 99.36%\n",
      "PC16: 0.08%, 99.44%\n",
      "PC17: 0.06%, 99.51%\n",
      "PC18: 0.06%, 99.56%\n",
      "PC19: 0.05%, 99.61%\n",
      "PC20: 0.04%, 99.65%\n",
      "PC21: 0.04%, 99.69%\n",
      "PC22: 0.03%, 99.72%\n",
      "PC23: 0.03%, 99.75%\n",
      "PC24: 0.02%, 99.77%\n",
      "PC25: 0.02%, 99.79%\n",
      "PC26: 0.02%, 99.8%\n",
      "PC27: 0.01%, 99.82%\n",
      "PC28: 0.01%, 99.83%\n",
      "PC29: 0.01%, 99.84%\n",
      "PC30: 0.01%, 99.86%\n",
      "PC31: 0.01%, 99.87%\n",
      "PC32: 0.01%, 99.88%\n",
      "PC33: 0.01%, 99.89%\n",
      "PC34: 0.01%, 99.89%\n",
      "PC35: 0.01%, 99.9%\n",
      "PC36: 0.01%, 99.91%\n",
      "PC37: 0.01%, 99.92%\n",
      "PC38: 0.01%, 99.92%\n",
      "PC39: 0.01%, 99.93%\n",
      "PC40: 0.01%, 99.93%\n",
      "PC41: 0.0%, 99.94%\n",
      "PC42: 0.0%, 99.94%\n",
      "PC43: 0.0%, 99.95%\n",
      "PC44: 0.0%, 99.95%\n",
      "PC45: 0.0%, 99.95%\n",
      "PC46: 0.0%, 99.96%\n",
      "PC47: 0.0%, 99.96%\n",
      "PC48: 0.0%, 99.96%\n",
      "PC49: 0.0%, 99.96%\n",
      "PC50: 0.0%, 99.97%\n",
      "PC51: 0.0%, 99.97%\n",
      "PC52: 0.0%, 99.97%\n",
      "PC53: 0.0%, 99.97%\n",
      "PC54: 0.0%, 99.98%\n",
      "PC55: 0.0%, 99.98%\n",
      "PC56: 0.0%, 99.98%\n",
      "PC57: 0.0%, 99.98%\n",
      "PC58: 0.0%, 99.98%\n",
      "PC59: 0.0%, 99.98%\n",
      "PC60: 0.0%, 99.99%\n",
      "PC61: 0.0%, 99.99%\n",
      "PC62: 0.0%, 99.99%\n",
      "PC63: 0.0%, 99.99%\n",
      "PC64: 0.0%, 99.99%\n",
      "PC65: 0.0%, 99.99%\n",
      "PC66: 0.0%, 99.99%\n",
      "PC67: 0.0%, 99.99%\n",
      "PC68: 0.0%, 99.99%\n",
      "PC69: 0.0%, 99.99%\n",
      "PC70: 0.0%, 99.99%\n",
      "PC71: 0.0%, 100.0%\n",
      "PC72: 0.0%, 100.0%\n",
      "PC73: 0.0%, 100.0%\n",
      "PC74: 0.0%, 100.0%\n",
      "PC75: 0.0%, 100.0%\n",
      "PC76: 0.0%, 100.0%\n",
      "PC77: 0.0%, 100.0%\n",
      "PC78: 0.0%, 100.0%\n",
      "PC79: 0.0%, 100.0%\n",
      "PC80: 0.0%, 100.0%\n",
      "PC81: 0.0%, 100.0%\n",
      "PC82: 0.0%, 100.0%\n",
      "PC83: 0.0%, 100.0%\n",
      "PC84: 0.0%, 100.0%\n",
      "PCA explained variance report saved to ./03_morphometrics_output/pca_explained_variance.txt\n",
      "\n",
      "--- Saving PCA model parameters, PC scores, and class labels ---\n",
      "  PCA Components shape: (84, 198)\n",
      "  PCA Mean shape: (198,)\n",
      "  PCA Explained Variance shape: (84,)\n",
      "  PCA Explained Variance Ratio shape: (84,)\n",
      "  Number of PCA components: 84\n",
      "  Original PCA Scores (PCs) shape: (84, 84)\n",
      "  Class Labels (type) length: 84\n",
      "PCA parameters saved to ./03_morphometrics_output/leaf_pca_model_parameters.h5\n",
      "Original PCA scores, class labels, AND original flattened coordinates saved to ./03_morphometrics_output/original_pca_scores_and_class_labels.h5\n",
      "\n",
      "--- Creating Morphospace Plot ---\n",
      "Morphospace plot saved to ./03_morphometrics_output/morphospace_plot.png\n",
      "\n",
      "--- All processing and saving completed ---\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "### LOAD IN MODULES ###\n",
    "#######################\n",
    "\n",
    "import cv2 # to install on mac: pip install opencv-python\n",
    "from scipy.interpolate import interp1d # for interpolating points\n",
    "from sklearn.decomposition import PCA # for principal component analysis\n",
    "from scipy.spatial import procrustes # for Procrustes analysis\n",
    "from scipy.spatial import ConvexHull # for convex hull (not used in provided code yet)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # for LDA (not used yet)\n",
    "from sklearn.metrics import confusion_matrix # for confusion matrix (not used yet)\n",
    "import scipy.stats as stats # for kruskal wallis test (not used yet)\n",
    "import statsmodels.stats.multitest as multitest # multiple test adjustment (not used yet)\n",
    "import itertools # for pairwise combinations (not used yet)\n",
    "from os import listdir # for retrieving files from directory\n",
    "from os.path import isfile, join # for retrieving files from directory\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np # for using arrays\n",
    "import math # for mathematical operations\n",
    "import pandas as pd # for using pandas dataframes\n",
    "import seaborn as sns # for plotting in seaborn\n",
    "from matplotlib.colors import LogNorm # for log scale (not used yet)\n",
    "import phate # for using PHATE (not used yet)\n",
    "import scprep # for using PHATE (not used yet)\n",
    "# import joblib # No longer needed for PCA saving, using h5py as requested\n",
    "import h5py # For saving large arrays and PCA model parameters\n",
    "import pickle # For saving Python objects (not strictly needed now, as leaf_indices is removed)\n",
    "import os # For path operations and directory creation\n",
    "\n",
    "#################\n",
    "### FUNCTIONS ###\n",
    "#################\n",
    "\n",
    "def angle_between(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    define a function to find the angle between 3 points anti-clockwise in degrees, p2 being the vertex\n",
    "    inputs: three angle points, as tuples\n",
    "    output: angle in degrees\n",
    "    \"\"\"\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    x3, y3 = p3\n",
    "    deg1 = (360 + math.degrees(math.atan2(x1 - x2, y1 - y2))) % 360\n",
    "    deg2 = (360 + math.degrees(math.atan2(x3 - x2, y3 - y2))) % 360\n",
    "    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)\n",
    "\n",
    "def rotate_points(xvals, yvals, degrees):\n",
    "    \"\"\"\"\n",
    "    define a function to rotate 2D x and y coordinate points around the origin\n",
    "    inputs: x and y vals (can take pandas dataframe columns) and the degrees (positive, anticlockwise) to rotate\n",
    "    outputs: rotated and y vals\n",
    "    \"\"\"\n",
    "    angle_to_move = 90-degrees\n",
    "    rads = np.deg2rad(angle_to_move)\n",
    "\n",
    "    new_xvals = xvals*np.cos(rads)-yvals*np.sin(rads)\n",
    "    new_yvals = xvals*np.sin(rads)+yvals*np.cos(rads)\n",
    "\n",
    "    return new_xvals, new_yvals\n",
    "\n",
    "def interpolation(x, y, number):\n",
    "    \"\"\"\n",
    "    define a function to return equally spaced, interpolated points for a given polyline\n",
    "    inputs: arrays of x and y values for a polyline, number of points to interpolate\n",
    "    ouputs: interpolated points along the polyline, inclusive of start and end points\n",
    "    \"\"\"\n",
    "    distance = np.cumsum(np.sqrt( np.ediff1d(x, to_begin=0)**2 + np.ediff1d(y, to_begin=0)**2 ))\n",
    "    distance = distance/distance[-1]\n",
    "\n",
    "    fx, fy = interp1d( distance, x ), interp1d( distance, y )\n",
    "\n",
    "    alpha = np.linspace(0, 1, number)\n",
    "    x_regular, y_regular = fx(alpha), fy(alpha)\n",
    "\n",
    "    return x_regular, y_regular\n",
    "\n",
    "def euclid_dist(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    define a function to return the euclidean distance between two points\n",
    "    inputs: x and y values of the two points\n",
    "    output: the eulidean distance\n",
    "    \"\"\"\n",
    "    return np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "\n",
    "def poly_area(x,y):\n",
    "    \"\"\"\n",
    "    define a function to calculate the area of a polygon using the shoelace algorithm\n",
    "    inputs: separate numpy arrays of x and y coordinate values\n",
    "    outputs: the area of the polygon\n",
    "    \"\"\"\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def gpa_mean(leaf_arr, landmark_num, dim_num):\n",
    "\n",
    "    \"\"\"\n",
    "    define a function that given an array of landmark data returns the Generalized Procrustes Analysis mean\n",
    "    inputs: a 3 dimensional array of samples by landmarks by coordinate values, number of landmarks, number of dimensions\n",
    "    output: an array of the Generalized Procrustes Analysis mean shape\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ref_ind = 0 # select arbitrary reference index to calculate procrustes distances to\n",
    "    ref_shape = leaf_arr[ref_ind, :, :] # select the reference shape\n",
    "\n",
    "    mean_diff = 10**(-30) # set a distance between means to stop the algorithm\n",
    "\n",
    "    old_mean = ref_shape # for the first comparison between means, set old_mean to an arbitrary reference shape\n",
    "\n",
    "    d = 1000000 # set d initially arbitraily high\n",
    "\n",
    "    while d > mean_diff: # set boolean criterion for Procrustes distance between mean to stop calculations\n",
    "\n",
    "        arr = np.zeros( ((len(leaf_arr)),landmark_num,dim_num) ) # empty 3D array: # samples, landmarks, coord vals\n",
    "\n",
    "        for i in range(len(leaf_arr)): # for each leaf shape\n",
    "\n",
    "            s1, s2, distance = procrustes(old_mean, leaf_arr[i]) # calculate procrustes adjusted shape to ref for current leaf\n",
    "            arr[i] = s2 # store procrustes adjusted shape to array\n",
    "\n",
    "        new_mean = np.mean(arr, axis=(0)) # calculate mean of all shapes adjusted to reference\n",
    "\n",
    "        s1, s2, d = procrustes(old_mean, new_mean) # calculate procrustes distance of new mean to old mean\n",
    "\n",
    "        old_mean = new_mean # set the old_mean to the new_mea before beginning another iteration\n",
    "\n",
    "    return new_mean\n",
    "\n",
    "# --- Configuration and Inputs ---\n",
    "\n",
    "# Input File Paths\n",
    "METADATA_FILE = \"./01_plowman_landmarks.csv\"\n",
    "IMAGE_DATA_DIR = \"./00_plowman_data/\"\n",
    "\n",
    "# Output Directory (will be created if it doesn't exist)\n",
    "# --- MODIFIED: Ensure consistency with 04_synthetic_data_generation.py ---\n",
    "OUTPUT_BASE_DIR = \"./03_morphometrics_output/\"\n",
    "os.makedirs(OUTPUT_BASE_DIR, exist_ok=True) # Ensure output directory exists\n",
    "\n",
    "# --- Parameters for Preprocessing ---\n",
    "HIGH_RES_INTERPOLATION_POINTS = 10000 # Initial high resolution outline points\n",
    "FINAL_PSEUDO_LANDMARKS_PER_SIDE = 50  # Number of equidistant points on each side (excluding the tip duplicate)\n",
    "                                     # Total pseudo-landmarks will be (FINAL_PSEUDO_LANDMARKS_PER_SIDE * 2) - 1\n",
    "\n",
    "# --- Parameters for Procrustes Analysis ---\n",
    "NUM_LANDMARKS = (FINAL_PSEUDO_LANDMARKS_PER_SIDE * 2) - 1 # Derived from above\n",
    "NUM_DIMENSIONS = 2                                       # For 2D coordinates\n",
    "\n",
    "# --- Parameters for PCA (Full Component Analysis) ---\n",
    "# This PCA is for explained variance analysis and later data augmentation.\n",
    "# It will calculate all possible components up to min(samples, features).\n",
    "\n",
    "# --- Parameters for Morphospace Visualization (2-Component PCA) ---\n",
    "MORPHOSPACE_PLOT_LENGTH = 10 # Plot length in inches\n",
    "MORPHOSPACE_PLOT_WIDTH = 10  # Plot width in inches\n",
    "MORPHOSPACE_PC1_INTERVALS = 20 # Number of PC1 intervals for eigenleaf grid\n",
    "MORPHOSPACE_PC2_INTERVALS = 6  # Number of PC2 intervals for eigenleaf grid\n",
    "MORPHOSPACE_HUE_COLUMN = \"type\" # Column in mdata to color points by for the morphospace plot\n",
    "EIGENLEAF_SCALE = 0.08 # Scaling of the inverse eigenleaves\n",
    "EIGENLEAF_COLOR = \"lightgray\" # Color of inverse eigenleaf\n",
    "EIGENLEAF_ALPHA = 0.5 # Alpha of inverse eigenleaf\n",
    "POINT_SIZE = 80 # Size of data points\n",
    "POINT_LINEWIDTH = 0 # Line width of data points (set to 0 for no edges)\n",
    "POINT_ALPHA = 0.6 # Alpha of the data points\n",
    "AXIS_LABEL_FONTSIZE = 12 # Font size of the x and y axis titles\n",
    "AXIS_TICK_FONTSIZE = 8 # Font size of the axis ticks\n",
    "FACE_COLOR = \"white\" # Color of the plot background\n",
    "GRID_ALPHA = 0.5 # Alpha of the grid\n",
    "\n",
    "# --- Parameters for Output Files ---\n",
    "GPA_MEAN_SHAPE_PLOT_FILENAME = \"gpa_mean_shape.png\"\n",
    "PCA_EXPLAINED_VARIANCE_REPORT_FILENAME = \"pca_explained_variance.txt\"\n",
    "MORPHOSPACE_PLOT_FILENAME = \"morphospace_plot.png\"\n",
    "\n",
    "# Specific filenames for saving PCA components, scores, and labels (using h5py)\n",
    "PCA_PARAMS_H5_FILENAME = \"leaf_pca_model_parameters.h5\"\n",
    "ORIGINAL_PCA_SCORES_AND_LABELS_H5_FILENAME = \"original_pca_scores_and_class_labels.h5\"\n",
    "CLASS_LABEL_COLUMN_FOR_SAVING = \"type\" # The column from mdata to use for class labels (e.g., 'type', 'cultivar', etc.)\n",
    "\n",
    "# E.g., FIGURE_DPI = 300 # Default DPI for saved figures\n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"Saving outputs to directory: {OUTPUT_BASE_DIR}\")\n",
    "\n",
    "########################\n",
    "### READ IN METADATA ###\n",
    "########################\n",
    "\n",
    "mdata = pd.read_csv(METADATA_FILE) # read in csv\n",
    "\n",
    "# Step 1: Sort the original DataFrame to ensure correct base/tip pairing\n",
    "# based on the original row 'index'.\n",
    "mdata_sorted = mdata.sort_values(by=['Label', 'index'])\n",
    "\n",
    "# Step 2: Group by 'Label' and aggregate to create 'base_x', 'base_y', 'tip_x', 'tip_y'\n",
    "new_df = mdata_sorted.groupby('Label').agg(\n",
    "    base_x=('X', lambda x: x.iloc[0]),\n",
    "    base_y=('Y', lambda x: x.iloc[0]),\n",
    "    tip_x=('X', lambda x: x.iloc[1]),\n",
    "    tip_y=('Y', lambda x: x.iloc[1])\n",
    ").reset_index()\n",
    "\n",
    "# Step 3: Rename the 'Label' column to 'file'\n",
    "mdata = new_df.rename(columns={'Label': 'file'})\n",
    "\n",
    "# Step 4: Extract the 'type' (class name) from the 'file' column\n",
    "# This takes the part of the string before the first underscore '_'\n",
    "mdata['type'] = mdata['file'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "#######################################\n",
    "### MAKE A LIST OF IMAGE FILE NAMES ###\n",
    "#######################################\n",
    "\n",
    "file_names = [f for f in listdir(IMAGE_DATA_DIR) if isfile(join(IMAGE_DATA_DIR, f))] # create a list of file names\n",
    "\n",
    "# Remove specific hidden files like .DS_Store (common on macOS)\n",
    "if '.DS_Store' in file_names:\n",
    "    file_names.remove('.DS_Store')\n",
    "\n",
    "file_names.sort() # sort the list of file names\n",
    "\n",
    "#####################################################################\n",
    "### INTERPOLATE POINTS CREATING PSEUDO-LANDMARKS AND PRE-PROCESS ###\n",
    "#####################################################################\n",
    "\n",
    "print(\"\\n--- Preprocessing Images and Interpolating Pseudo-Landmarks ---\")\n",
    "# an array to store pseudo-landmarks\n",
    "cult_cm_arr = np.zeros((len(mdata), NUM_LANDMARKS, NUM_DIMENSIONS))\n",
    "\n",
    "# for each leaf . . .\n",
    "for lf in range(len(mdata)):\n",
    "\n",
    "    curr_image = mdata[\"file\"][lf] # select the current image\n",
    "    # print(f\"Processing leaf {lf+1}/{len(mdata)}: {curr_image}\") # Optional: progress indicator\n",
    "\n",
    "    img = cv2.bitwise_not(cv2.cvtColor(cv2.imread(os.path.join(IMAGE_DATA_DIR, curr_image)), cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(img,\n",
    "        cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    x_conts = []\n",
    "    y_conts = []\n",
    "    areas_conts = []\n",
    "    for c in contours:\n",
    "        x_vals = []\n",
    "        y_vals = []\n",
    "        for i in range(len(c)):\n",
    "            x_vals.append(c[i][0][0])\n",
    "            y_vals.append(c[i][0][1])\n",
    "        area = (max(x_vals) - min(x_vals))*(max(y_vals) - min(y_vals))\n",
    "        x_conts.append(x_vals)\n",
    "        y_conts.append(y_vals)\n",
    "        areas_conts.append(area)\n",
    "\n",
    "    area_inds = np.flip(np.argsort(areas_conts))\n",
    "    sorted_x_conts = np.array(x_conts, dtype=object)[area_inds][0:]\n",
    "    sorted_y_conts = np.array(y_conts, dtype=object)[area_inds][0:]\n",
    "\n",
    "    high_res_x, high_res_y = interpolation(np.array(sorted_x_conts[0], dtype=np.float32),\n",
    "                                            np.array(sorted_y_conts[0], dtype=np.float32), HIGH_RES_INTERPOLATION_POINTS)\n",
    "\n",
    "    base_pt = np.array((mdata[\"base_x\"][lf], mdata[\"base_y\"][lf]))\n",
    "    tip_pt = np.array((mdata[\"tip_x\"][lf], mdata[\"tip_y\"][lf]))\n",
    "\n",
    "    base_dists = []\n",
    "    tip_dists = []\n",
    "\n",
    "    for pt in range(len(high_res_x)):\n",
    "        ed_base = euclid_dist(base_pt[0], base_pt[1], high_res_x[pt], high_res_y[pt])\n",
    "        ed_tip = euclid_dist(tip_pt[0], tip_pt[1], high_res_x[pt], high_res_y[pt])\n",
    "        base_dists.append(ed_base)\n",
    "        tip_dists.append(ed_tip)\n",
    "\n",
    "    base_ind = np.argmin(base_dists)\n",
    "    tip_ind = np.argmin(tip_dists)\n",
    "\n",
    "    high_res_x = np.concatenate((high_res_x[base_ind:],high_res_x[:base_ind]))\n",
    "    high_res_y = np.concatenate((high_res_y[base_ind:],high_res_y[:base_ind]))\n",
    "\n",
    "    tip_ind = tip_ind-base_ind\n",
    "    base_ind = base_ind-base_ind\n",
    "\n",
    "    lf_contour = np.column_stack((high_res_x, high_res_y))\n",
    "\n",
    "    left_inter_x, left_inter_y = interpolation(lf_contour[base_ind:tip_ind+1,0],lf_contour[base_ind:tip_ind+1,1],FINAL_PSEUDO_LANDMARKS_PER_SIDE)\n",
    "    right_inter_x, right_inter_y = interpolation(lf_contour[tip_ind:,0],lf_contour[tip_ind:,1],FINAL_PSEUDO_LANDMARKS_PER_SIDE)\n",
    "\n",
    "    left_inter_x = np.delete(left_inter_x, -1)\n",
    "    left_inter_y = np.delete(left_inter_y, -1)\n",
    "\n",
    "    lf_pts_left = np.column_stack((left_inter_x, left_inter_y))\n",
    "    lf_pts_right = np.column_stack((right_inter_x, right_inter_y))\n",
    "    lf_pts = np.row_stack((lf_pts_left, lf_pts_right))\n",
    "\n",
    "    tip_point = lf_pts[FINAL_PSEUDO_LANDMARKS_PER_SIDE-1,:]\n",
    "    base_point = lf_pts[0,:]\n",
    "\n",
    "    ang = angle_between(tip_point, base_point, (base_point[0]+1,base_point[1]) )\n",
    "\n",
    "    rot_x, rot_y = rotate_points(lf_pts[:,0], lf_pts[:,1], ang)\n",
    "    rot_pts = np.column_stack((rot_x, rot_y))\n",
    "\n",
    "    cult_cm_arr[lf,:,:] = rot_pts\n",
    "\n",
    "##########################\n",
    "### CALCULATE GPA MEAN ###\n",
    "##########################\n",
    "\n",
    "print(\"\\n--- Calculating GPA Mean ---\")\n",
    "mean_shape = gpa_mean(cult_cm_arr, NUM_LANDMARKS, NUM_DIMENSIONS)\n",
    "\n",
    "################################\n",
    "### ALIGN LEAVES TO GPA MEAN ###\n",
    "################################\n",
    "\n",
    "print(\"--- Aligning Leaves to GPA Mean ---\")\n",
    "proc_arr = np.zeros(np.shape(cult_cm_arr))\n",
    "\n",
    "for i in range(len(cult_cm_arr)):\n",
    "    s1, s2, distance = procrustes(mean_shape, cult_cm_arr[i, :, :])\n",
    "    proc_arr[i] = s2\n",
    "\n",
    "#### VISUALIZE GPA ALIGNED SHAPES AND MEAN\n",
    "print(\"--- Visualizing GPA Aligned Shapes ---\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(len(proc_arr)):\n",
    "    plt.plot(proc_arr[i, :, 0], proc_arr[i, :, 1], c=\"k\", alpha=0.08)\n",
    "\n",
    "plt.plot(np.mean(proc_arr, axis=0)[:, 0], np.mean(proc_arr, axis=0)[:, 1], c=\"magenta\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Procrustes Aligned Leaf Shapes and GPA Mean\")\n",
    "\n",
    "plt.savefig(os.path.join(OUTPUT_BASE_DIR, GPA_MEAN_SHAPE_PLOT_FILENAME))\n",
    "plt.close()\n",
    "print(f\"GPA mean shape plot saved to {os.path.join(OUTPUT_BASE_DIR, GPA_MEAN_SHAPE_PLOT_FILENAME)}\")\n",
    "\n",
    "\n",
    "#################################################\n",
    "### FIRST, CALCULATE PERCENT VARIANCE ALL PCs ###\n",
    "#################################################\n",
    "\n",
    "print(\"\\n--- Performing Full PCA and Generating Explained Variance Report ---\")\n",
    "\n",
    "# use the reshape function to flatten to 2D\n",
    "flat_arr = proc_arr.reshape(np.shape(proc_arr)[0],\n",
    "                            np.shape(proc_arr)[1] * np.shape(proc_arr)[2])\n",
    "\n",
    "# Determine the maximum number of principal components possible: min(n_samples, n_features)\n",
    "max_pc_components = min(flat_arr.shape[0], flat_arr.shape[1])\n",
    "\n",
    "# Initialize PCA to calculate all possible PCs for full variance analysis\n",
    "pca = PCA(n_components=max_pc_components)\n",
    "PCs = pca.fit_transform(flat_arr) # fit a PCA for all data\n",
    "\n",
    "# Generate and save explained variance report\n",
    "pca_explained_variance_filepath = os.path.join(OUTPUT_BASE_DIR, PCA_EXPLAINED_VARIANCE_REPORT_FILENAME)\n",
    "with open(pca_explained_variance_filepath, 'w') as f:\n",
    "    f.write(\"PCA Explained Variance Report:\\n\")\n",
    "    f.write(f\"Total Samples: {flat_arr.shape[0]}\\n\")\n",
    "    f.write(f\"Total Features (landmarks * dimensions): {flat_arr.shape[1]}\\n\")\n",
    "    f.write(f\"Number of PCs Calculated: {pca.n_components_}\\n\\n\")\n",
    "\n",
    "    f.write(\"PC: var, overall\\n\")\n",
    "    for i in range(len(pca.explained_variance_ratio_)):\n",
    "        pc_variance = round(pca.explained_variance_ratio_[i] * 100, 2)\n",
    "        cumulative_variance = round(pca.explained_variance_ratio_.cumsum()[i] * 100, 2)\n",
    "        line = f\"PC{i+1}: {pc_variance}%, {cumulative_variance}%\\n\"\n",
    "        print(line.strip()) # Also print to console\n",
    "        f.write(line)\n",
    "print(f\"PCA explained variance report saved to {pca_explained_variance_filepath}\")\n",
    "\n",
    "# --- Save PCA Model Parameters, PC Scores, and Class Labels ---\n",
    "print(\"\\n--- Saving PCA model parameters, PC scores, and class labels ---\")\n",
    "\n",
    "# 1. Extract information from the PCA model and original data\n",
    "pca_components = pca.components_\n",
    "pca_mean = pca.mean_\n",
    "pca_explained_variance = pca.explained_variance_\n",
    "pca_explained_variance_ratio = pca.explained_variance_ratio_\n",
    "n_pca_components = pca.n_components_\n",
    "\n",
    "print(f\"  PCA Components shape: {pca_components.shape}\")\n",
    "print(f\"  PCA Mean shape: {pca_mean.shape}\")\n",
    "print(f\"  PCA Explained Variance shape: {pca_explained_variance.shape}\")\n",
    "print(f\"  PCA Explained Variance Ratio shape: {pca_explained_variance_ratio.shape}\")\n",
    "print(f\"  Number of PCA components: {n_pca_components}\")\n",
    "print(f\"  Original PCA Scores (PCs) shape: {PCs.shape}\")\n",
    "print(f\"  Class Labels ({CLASS_LABEL_COLUMN_FOR_SAVING}) length: {len(mdata[CLASS_LABEL_COLUMN_FOR_SAVING])}\")\n",
    "\n",
    "# 2. Save the PCA model parameters to an HDF5 file\n",
    "pca_params_filepath = os.path.join(OUTPUT_BASE_DIR, PCA_PARAMS_H5_FILENAME)\n",
    "with h5py.File(pca_params_filepath, 'w') as f:\n",
    "    f.create_dataset('components', data=pca_components, compression=\"gzip\")\n",
    "    f.create_dataset('mean', data=pca_mean, compression=\"gzip\")\n",
    "    f.create_dataset('explained_variance', data=pca_explained_variance, compression=\"gzip\")\n",
    "    f.create_dataset('explained_variance_ratio', data=pca_explained_variance_ratio, compression=\"gzip\")\n",
    "    f.attrs['n_components'] = n_pca_components\n",
    "print(f\"PCA parameters saved to {pca_params_filepath}\")\n",
    "\n",
    "# 3. Save original PCA scores (PCs) and class labels to an HDF5 file\n",
    "pca_scores_labels_filepath = os.path.join(OUTPUT_BASE_DIR, ORIGINAL_PCA_SCORES_AND_LABELS_H5_FILENAME)\n",
    "with h5py.File(pca_scores_labels_filepath, 'w') as f:\n",
    "    f.create_dataset('pca_scores', data=PCs, compression=\"gzip\")\n",
    "    # Convert labels to a numpy array of byte strings for HDF5 compatibility\n",
    "    f.create_dataset('class_labels', data=np.array(mdata[CLASS_LABEL_COLUMN_FOR_SAVING]).astype('S'), compression=\"gzip\")\n",
    "    # --- ADDED: Save the original flattened coordinates ---\n",
    "    f.create_dataset('original_flattened_coords', data=flat_arr, compression=\"gzip\")\n",
    "print(f\"Original PCA scores, class labels, AND original flattened coordinates saved to {pca_scores_labels_filepath}\")\n",
    "\n",
    "\n",
    "##########################\n",
    "### CREATE MORPHOSPACE ###\n",
    "##########################\n",
    "\n",
    "print(\"\\n--- Creating Morphospace Plot ---\")\n",
    "\n",
    "# The flat_arr is already prepared from the previous full PCA step.\n",
    "# flat_arr = proc_arr.reshape(np.shape(proc_arr)[0],\n",
    "#                             np.shape(proc_arr)[1] * np.shape(proc_arr)[2])\n",
    "\n",
    "# Perform PCA specifically for morphospace visualization (2 components)\n",
    "morphospace_pca = PCA(n_components=2)\n",
    "morphospace_PCs = morphospace_pca.fit_transform(flat_arr)\n",
    "\n",
    "# Add the 2-component PCA results to the mdata DataFrame\n",
    "mdata[\"PC1\"] = morphospace_PCs[:, 0]\n",
    "mdata[\"PC2\"] = morphospace_PCs[:, 1]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(MORPHOSPACE_PLOT_LENGTH, MORPHOSPACE_PLOT_WIDTH))\n",
    "plt.gca().set_facecolor(FACE_COLOR)\n",
    "plt.gca().set_axisbelow(True)\n",
    "\n",
    "# Create PC intervals for plotting inverse eigenleaves\n",
    "PC1_vals = np.linspace(np.min(PCs[:, 0]), np.max(PCs[:, 0]), MORPHOSPACE_PC1_INTERVALS)\n",
    "PC2_vals = np.linspace(np.min(PCs[:, 1]), np.max(PCs[:, 1]), MORPHOSPACE_PC2_INTERVALS)\n",
    "\n",
    "# Plot inverse eigenleaves (the background grid shapes)\n",
    "for i in PC1_vals:\n",
    "    for j in PC2_vals:\n",
    "        inv_leaf = morphospace_pca.inverse_transform(np.array([i, j]))\n",
    "        inv_leaf_coords = inv_leaf.reshape(NUM_LANDMARKS, NUM_DIMENSIONS) # Reshape back to 2D points\n",
    "\n",
    "        inv_x = inv_leaf_coords[:, 0]\n",
    "        inv_y = inv_leaf_coords[:, 1]\n",
    "\n",
    "        plt.fill(inv_x * EIGENLEAF_SCALE + i, inv_y * EIGENLEAF_SCALE + j,\n",
    "                 c=EIGENLEAF_COLOR, alpha=EIGENLEAF_ALPHA)\n",
    "\n",
    "# Plot the data points on top of the morphospace\n",
    "sns.scatterplot(data=mdata, x=\"PC1\", y=\"PC2\", hue=MORPHOSPACE_HUE_COLUMN,\n",
    "                s=POINT_SIZE, linewidth=POINT_LINEWIDTH, alpha=POINT_ALPHA)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9})\n",
    "\n",
    "# Customize axis labels using explained variance from the FULL PCA\n",
    "# Ensure `pca.explained_variance_ratio_` is accessible from the full PCA.\n",
    "xlab = f\"PC1, {round(pca.explained_variance_ratio_[0] * 100, 1)}%\"\n",
    "ylab = f\"PC2, {round(pca.explained_variance_ratio_[1] * 100, 1)}%\"\n",
    "plt.xlabel(xlab, fontsize=AXIS_LABEL_FONTSIZE)\n",
    "plt.ylabel(ylab, fontsize=AXIS_LABEL_FONTSIZE)\n",
    "plt.xticks(fontsize=AXIS_TICK_FONTSIZE)\n",
    "plt.yticks(fontsize=AXIS_TICK_FONTSIZE)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(OUTPUT_BASE_DIR, MORPHOSPACE_PLOT_FILENAME), bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"Morphospace plot saved to {os.path.join(OUTPUT_BASE_DIR, MORPHOSPACE_PLOT_FILENAME)}\")\n",
    "\n",
    "print(\"\\n--- All processing and saving completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d9134-7cb6-4c0e-a7ae-09b2a7366546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
